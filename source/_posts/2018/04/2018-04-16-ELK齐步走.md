---
title: ELK齐步走
author: HoldDie
img: 
top: false
cover: false
coverImg: 
toc: true
mathjax: true
tags:
  - Logstash
  - ElasticSearch
  - Kibana
date: 2018-04-16 21:32:16
password:
summary:  
categories: ELK
---

ELK 简单部署使用，记录一哈。



为什么使用 logstash，方便为 ES 导入数据：

安装启动错误：因为是在win系统下，%CLASSPATH% 路径带有空格，故需要修改启动脚本。

```shell
%JAVA%  %JAVA_OPTS% -cp "%CLASSPATH%" org.logstash.Logstash %*

endlocal

goto :eof
:concat
IF not defined CLASSPATH (
  set CLASSPATH="%~1"
) ELSE (
  set CLASSPATH=%CLASSPATH%;"%~1"
)
```

启动语句：

```shell
logstash -e 'input { stdin { } } output { stdout {} }'
```

安装插件 `jdbc`  ,看网上有很多还要安装`ruby`环境，我只执行这个命令就可以

```shell
./plugin install logstash-input-jdbc
```

然后设置 `mysql.conf` 全量脚本：

```coffeescript
input {
    stdin {
    }
    jdbc {
    # 数据库
    jdbc_connection_string => "jdbc:mysql://10.20.69.237:3306/b2b2c2"
    # 用户名密码
    jdbc_user => "admin"
    jdbc_password => "admin123"
    # jar包的位置
    jdbc_driver_library => "E:\Elastic\logstash-6.2.3\bin\config-mysql\mysql-connector-java-5.1.45.jar"
    # mysql的Driver
    jdbc_driver_class => "com.mysql.jdbc.Driver"
    jdbc_paging_enabled => "true"
    jdbc_page_size => "50000"
    #statement_filepath => "config-mysql/test02.sql"
    statement => "select * from test02"
    schedule => "* * * * *"
    #索引的类型
	type => "test02"
    }
}

filter {
    json {
        source => "message"
        remove_field => ["message"]
    }
}

output {
    elasticsearch {
        hosts => "10.20.69.235:9200"
        # index名
        index => "test01"
        # 需要关联的数据库中有有一个id字段，对应索引的id号
        document_id => "%{id}"
    }
    stdout {
        codec => json_lines
    }
}
```

因为使用的是mysql，故需要制定一个 `mysql` 的 `jar` 文件

最后执行命令：

```shell
logstash -f config-mysql/mysql.conf
```

然后根据后台日志时间，我们初步确认就是 一分钟更新一个 数据，进行进行同步跟新，好流弊。

接下来我们自己设置一下增量设置：

```shell
input {  
    stdin {  
    }  
    jdbc {  
      # mysql 数据库链接,test为数据库名  
      jdbc_connection_string => "jdbc:mysql://127.0.0.1:3306/test"  
      # 用户名和密码  
      jdbc_user => "root"  
      jdbc_password => "root"  
      # 驱动  
      jdbc_driver_library => "G:\Developer\Elasticsearch5.5.1\ES5\logstash-5.5.1\bin\mysql\mysql-connector-java-5.1.9.jar"  
      # 驱动类名  
      jdbc_driver_class => "com.mysql.jdbc.Driver"  
  
      #处理中文乱码问题  
      codec => plain { charset => "UTF-8"}  
      #使用其它字段追踪，而不是用时间  
      use_column_value => true  
      #追踪的字段  
      tracking_column => id  
      record_last_run => true  
     #上一个sql_last_value值的存放文件路径, 必须要在文件中指定字段的初始值  
     last_run_metadata_path => "G:\Developer\Elasticsearch5.5.1\ES5\logstash-5.5.1\bin\mysql\station_parameter.txt"  
     #开启分页查询  
     jdbc_paging_enabled => true  
     jdbc_page_size => 300  
        
      # 执行的sql 文件路径+名称  
      statement_filepath => "G:\Developer\Elasticsearch5.5.1\ES5\logstash-5.5.1\bin\mysql\jdbc.sql"  
      # 设置监听间隔  各字段含义（由左至右）分、时、天、月、年，全部为*默认含义为每分钟都更新  
      schedule => "* * * * *"  
      # 索引类型  
      type => "jdbc"  
  
    }  
}  
  
filter {  
    json {  
        source => "message"  
        remove_field => ["message"]  
    }  
}  
  
output {  
    elasticsearch {  
        # ES的IP地址及端口  
        hosts => ["localhost:9200"]  
        # 索引名称  
        index => "article"  
        # 自增ID  
        document_id => "%{id}"  
    }  
    stdout {  
        # JSON格式输出  
        codec => json_lines  
    }  
}  
```

对于参数的解释：

```shell
//是否记录上次执行结果, 如果为真,将会把上次执行到的 tracking_column 字段的值记录下来,保存到 last_run_metadata_path 指定的文件中  
record_last_run => true  
  
//是否需要记录某个column 的值,如果 record_last_run 为真,可以自定义我们需要 track 的 column 名称，此时该参数就要为 true. 否则默认 track 的是 timestamp 的值.  
use_column_value => true  
  
//如果 use_column_value 为真,需配置此参数. track 的数据库 column 名,该 column 必须是递增的.比如：ID.  
tracking_column => MY_ID  
  
//指定文件,来记录上次执行到的 tracking_column 字段的值  
//比如上次数据库有 10000 条记录,查询完后该文件中就会有数字 10000 这样的记录,下次执行 SQL 查询可以从 10001 条处开始.  
//我们只需要在 SQL 语句中 WHERE MY_ID > :last_sql_value 即可. 其中 :last_sql_value 取得就是该文件中的值(10000).  
last_run_metadata_path => "G:\Developer\Elasticsearch5.5.1\ES5\logstash-5.5.1\bin\mysql\station_parameter.txt"  
  
  
//是否清除 last_run_metadata_path 的记录,如果为真那么每次都相当于从头开始查询所有的数据库记录  
clean_run => false  
  
//是否将 column 名称转小写  
lowercase_column_names => false  
  
//存放需要执行的 SQL 语句的文件位置  
statement_filepath => "G:\Developer\Elasticsearch5.5.1\ES5\logstash-5.5.1\bin\mysql\jdbc.sql"  
```



```
E:/Elastic/logstash-6.2.3/bin/config-mysql
```

### 那些年走过的坑

ES 最新的 6.x 版本不支持一个索引下有多个type了

#### 错误一

描述：

```
Rejecting mapping update to [house] as the final mapping would have more than 1 type: [housetype, do
```

原因：在创建索引映射的时候的索引类型设置为 housetype，但是`logstash` 在同步的时候会自动创建一个doc类型的index，所以引起冲突，解决方式可以修改自己的映射为doc，这样就不会发生冲突了

#### 错误二

描述：每次启动都是莫名奇妙的其他错误，但是自己mysql的启动就是没有问题，

原因：对于启动的文件的名称也有问题，貌似目前的问题就是使用的文件名不同了

