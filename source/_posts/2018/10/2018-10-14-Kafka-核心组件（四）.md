---
title: Kafka-核心组件（四）
author: HoldDie
tags: [Kafka,消息中间件]
top: false
date: 2018-10-14 19:43:41
categories: Kafka
---



> 我脚下的这片土地，埋藏了万千战士的英魂，我因此而颤抖。 ——笑道应有思

> Kafka核心功能模块，主要包括**延迟操作组件**、**控制器**、**协调器**、**网络通信**、**日志管理器**、**副本管理器**、**动态配置管理器**及**心跳检测**。

### 1 延迟操作组件

> 延迟操作组件辅助其他组件完成相应功能：
>
> - 协助客户端处理创建主题操作
> - 协助组协调器 (GroupCoordinator) 处理JoinGroupRequet 和 HeartbeatRequest请求
> - 协助副本管理器 (ReplicaManager) 处理ProduceRequest和FetchRequest请求

#### 1.1 DelayedOperation

> 定义：将一些不立即执行而要等待满足一定条件才能触发完成的操作成为延迟操作，抽象类 `DelayedOperation`

- DelayedOperation 是一个基于事件启动有失效时间的 TimerTask。
- TimerTask 实现了 Runnable 接口，维护了一个 TimerTaskEntry 对象，TimerTaskEntry 绑定了一个 TimerTask
- TimerTaskEntry 被添加到 TimerTaskList 中，TimerTaskList 是一个环形双向链表，按失效时间排序。
- DelayedOperation 是一个抽象类，具体的延迟操作类继承于该抽象类，分别用来协助相应组件对不同的请求完成延迟处理操作
- DelayedOperation 只有一个 AtomicBoolean 类型的 completed 属性，用来控制某个延迟操作。在延迟时间（delayMs）内，onComplete() 方法只被调用一次。

![DelayedOperation抽象类图](../../../../../../../../images/2018/10/DelayedOperation抽象类图.png)

#### 核心方法

- tryComplete()方法

  ：

  - 一个抽象方法，由子类来实现，负责检测执行条件是否满足。
  - 若满足执行条件，则调用 forceComplete() 方法完成延迟操作。

- forceCompete()方法

  ：

  - 该方法在条件满足时，检测延迟任务是否未被执行。
  - 若未被执行，则先调用 TimerTask.cancel() 方法解除该延迟操作与 TimerTaskEntry 的绑定，将该延迟操作从 TimerTaskList 链表中移除，
  - 然后调用 onComplete() 方法让延迟操作执行完成。
  - 通过 completed 的 CAS 原子操作（completed.compareAndSet），可以保证并发操作时只有第一个调用该方法的线程能够顺利调用 onComplete() 完成延迟操作，其他线程获取的 completed 属性为 false，即不会调用 onComplete() 方法，这就保证了 onComplete() 只会被调用一次。

- onComplete() 方法

  ：

  - 是一个抽象方法，由子类来实现，执行延迟操作满足执行条件后需要执行的实际业务逻辑。
  - 例如，DelayedProduce 和 DelayedFetch 都是在该方法内调用 responseCallback 向客户端做出响应。

- safeTryComplete() 方法

  ：

  - 以 synchronized 同步锁调用 onComplete() 方法，供外部调用。

- onExpiration() 方法

  ：

  - 也是一个抽象方法，由子类来实现当延迟操作已达失效时间的相应逻辑处理。
  - Kafka 通过 SystemTimer 来定期检测请求是否超时。
  - SystemTimer 是 Kafka 实现的底层基于层级时间轮和 DelayQueue 定时器，维护了一个 newFixedThreadPool 线程池，用于提交相应的线程执行。
  - 例如，当检测到延迟操作已失效时则将延迟操作提交到该线程池，即执行线程的 run() 方法的逻辑。DelayedOperation 覆盖了 TimerTask 的 run() 方法，在该方法中先调用 forceCompete() 方法，当该方法返回 true 后再调用 onExpiration() 方法。

> Kafka 当前的设计 onComplete() 方法是向客户端做出响应的唯一出口，当延迟操作达到失效时间时也是先执行 forceCompete() 方法，让 onComplete() 方法执行之后再调用 onExpiration() 方法，在 onExpiration() 方法中仅是进行相应的过期信息收集之类的操作。

![DelayedOperation各方法请求流程](../../../../../../../../images/2018/10/DelayedOperation个方法请求流程.png)

#### 1.2 DelayedOperationPurgatory

- 一个对 DelayedOperation 管理的辅助类，

- Purgatory 以泛型的形式将一个 DelayedOperation 添加到其内部维护的 Pool[Any, Watchers] 类型 watchersForKey 对象中，同时将 DelayedOperation 添加到 SystemTimer 中。

- Watchers 是 Purgatory 的内部类，底层是一个 ConcurrentLinkedQueue，该类定义了一个 ConcurrentLinkedQueue 类型的 operations 属性，用于保存 DelayedOperation。

- 从 Watchers 类名可以看出，该类的作用就是对 DelayedOperation 进行监视。

  - watch() 方法

    ：

    - 用于将 DelayedOperation 添加到 operations 集合中。

  - tryCompleteWatched() 方法

    ：

    - 用于迭代 operations 集合中的 DelayedOperation，通过 DelayedOperation.isCompleted 检测该 DelayedOperation 是否已执行完成。
    - 若已执行完成，则从 operations 集合中移除该 DelayedOperation。
    - 否则调用 DelayedOperation.safeTryComplete() 方法尝试让该 DelayedOperation 执行完成，
    - 若执行完成，即 safeTryComplete() 方法返回 true，则将该 DelayedOperation 从 operations 集合中移除。
    - 最后检测 operations 集合是否为空，如果 operations 为空，则表示该 operations 所关联的 DelayedOperation 已全部执行完成
    - 因此将该 Watchers 从 Purgatory 的 Pool 中移除。

  - purgeCompleted() 方法

    ：

    - 与 tryCompleteWatched() 方法基本功能类似，
    - 区别在于 purgeCompleted() 方法只单纯地将该 operations 集合中已完成的 DelayedOperation 移除，**对未完成的 DelayedOperation 并不尝试将其执行完成**。

  - 我们可以简单地将 Purgatory 与 Spring Quartz 类比，这样对 Purgatory 的作用就不难理解了。Purgatory 相当于 Quartz 的 SchedulerFactoryBean，而 DelayedOperation 相当于 ScheduleFactoryBean 所管理的具体 Schedule，由 Purgatory 负责调度。只不过 Purgatory 除了管理调度 DelayedOperation 之外，还负责 DelayedOperation 超时的管理。下面简要介绍 Purgatory 的两个主要方法。

  - tryCompleteElseWatch ()方法

    ：

    - 该方法首先调用待检测的 DelayedOperation.safeTryComplete() 方法，检测是否能执行完成，
    - 若未执行完成，则迭代 watchersForKey 对应的 DelayedOperation 检测 DelayedOperation 是否已完成，
    - 若未完成，则将其添加到 Watchers 中。添加完成后，再调用 safeTryComplete() 方法再次尝试让 DelayedOpeartion 执行完成，
    - 若还是未完成，再将其添加到 SystemTimer 中。添加完后再次检测是否执行完成，
    - 若已执行完成则将其从 SystemTimer 中移除。
    - 可以看到，在整个操作逻辑中多次执行 safeTryComplete() 方法以及多次检测是否已完成，是以防在操作过程中可能已被其他线程触发执行完成。
    - 同时在将 DelayedOperation 添加到 Watchers 操作时并没有将原来的 Key 清理掉，这是因为 Purgatory 在启动时会同时启动一个 ExpiredOperationReaper 线程，该线程除了推进时间轮的指针外还会定期清理 watchersForKey 已完成的 DelayedOperation。

  - checkAndComplete() 方法

    ：

    - 根据所传入的 Key，检测该 Key 对应的 Watchers 是否执行完成，若未完成，再调用 Watchers.tryCompleteWatched() 方法进行处理。

> Purgatory 对 DelayedOperation 的管理是通过 Watchers 来完成的，通过 Watchers 调用 DelayedOperation 相应的方法，让 DelayedOperation 要么在 delayMs 时间内完成，要么超时。

#### 1.3 DelayedProduce

- DelayedProduce 的作用就是协助副本管理器在 acks 为−1的场景时，延迟回调 responseCallback 向生产者做出响应。
- 具体表现在当消息追加到分区 Leader 副本之后，该分区各 Follower 副本完成了与 Leader 副本消息同步之后再回调 responseCallback 给生产者。

##### 1.3.1 DelayedProduce 处理逻辑

- （1）写操作发生异常。更新该分区的 ProducePartitionStatus.PartitionResponse.errorCode，同时更新 acksPending=false。
- （2）当分区 Leader 副本发生迁移时。此时也需要更新该分区的 ProducePartitonStatus 和 acksPending=false。
- （3）ISR 副本同步完成，Leader 副本的 HW（HighWatermark）已大于 requiredOffset。通过 Partition.checkEnoughReplicasReachOffset（status.requiredOffset） 处理后会修改 DelayedProduce 初始化时对 PartitionResponse.errorCode 所设置的默认值。
- DelayedProduce.tryComplete() 方法检测 DelayedProduce 是否满足执行条件，DelayedProduce 需要在本次请求对应的所有分区都满足条件之后才调用 forceComplete() 方法来完成延迟操作。

#### 1.4 DelayedFetch

- DelayedProduce 是在 ProduceRequest 处理中对生产者发送消息的延迟操作，自然 DelayedFetch 就是在 FetchRequest 处理时进行的延迟操作。
- 在 Kafka 中只有消费者或是 Follower 副本会发起 FetchReuqest 请求。
- FecthRequest 是由 KafkaApis.handleFetchRequest() 方法处理的，在该方法中会调用 ReplicaManager.fetchMessages() 方法从相应分区的 Leader 副本拉取消息。
- 在 ReplicaManager.fetchMessages() 方法中会创建 DelayedFetch 延迟操作。

#### 1.5 DelayedJoin

- DelayedJoin 是协助组协调器在消费组准备平衡操作时进行相应的处理。
- 当消费组的状态转换为 PreparingRebalance 时，即准备进行平衡操作，在组协调器的 prepareRebalance() 方法中会创建一个 DelayedJoin 对象，并交由 DelayedOperationPurgatory 负责监视管理。
- 在消费组进行平衡操作时之所以需要 DelayedJoin 处理，是为了让组协调器等待当前消费组下所有的消费者都请求加入消费组，即发起了 JoinGroupRequest 请求。
- 每次组协调器处理完 JoinGroupRequest 时都会检测 DelayedJoin 是否满足了完成执行的条件。

#### 1.6 DelayedHeartbeat

- DelayedHeartbeat 用于协助消费者与组协调器心跳检测相关的延迟操作，DelayedHeartbeat 相关功能的实现是调用 GroupCoordinator 的相应方法来完成的。

#### 1.7 DelayedCreateTopics

- 在创建主题时，需要为主题的每个分区分配到 Leader 之后，才调用回调函数将创建主题结果返回给客户端。
- DelayedCreateTopics 延迟操作等待该主题的所有分区副本分配到 Leader 或是等待超时后调用回调函数返回给客户端。

### 2 控制器

#### 2.0 基础概念

- 在启动 Kafka 集群时，每一个代理都会实例化并启动一个 KafkaController，并将该代理的 brokerId 注册到 ZooKeeper 的相应节点当中。
- Kafka 集群中各代理会根据选举机制选出其中一个代理作为 Leader，即 Leader 控制器（本书简称之为控制器，在没有特殊说明情况下，控制器均指 Leader 控制器）。
- 当控制器发生宕机后其他代理再次竞选出新的控制器。
- 控制器负责主题的创建与删除、分区和副本的管理以及代理故障转移处理等。
  - 当一个代理被选举成为控制器时，该代理对应的 KafkaController 就会注册（Register）控制器相应的操作权限，同时标记自己是 Leader。
  - 当代理不再成为控制器时，就要注销掉（DeRegister）相应的权限。实现这些功能的程序入口是在 Kafka 核心 core 工程下的 kafka.controller.KafkaController 类。

核心关键词：

- ```
  controller_epoch
  ```

  ：

  - 用于记录控制器发生变更次数，即记录当前的控制器是第几代控制器（本书中我们称之为控制器轮值次数）。
  - 初始值为0，当控制器发生变更时，每选出一个新的控制器需将该字段加1，每个向控制器发送的请求都会带上该字段，
  - 如果请求的 `controller_epoch`的值小于内存中`controller_epoch`的值，则认为这个请求是向已过期的控制器发送的请求，那么本次请求就是一个无效的请求。
  - 若该值大于内存中`controller_epoch`的值，则说明已有新的控制器当选了。
  - 通过该值来保证集群控制器的唯一性，进而保证相关操作一致性。该字段对应 ZooKeeper 的 `controller_epoch`节点，通过登录 ZooKeeper 客户端执行`get/controller_epoch`命令，可以查看该字段对应的值。

- zkVersion：

  - 作用类似数据库乐观锁，用于更新 ZooKeeper 路径下相应元数据信息，如controller epoch，ISR 信息等。

- ```
  leader_epoch
  ```

  ：

  - 分区 Leader 更新次数。`controller_epoch`是相对代理而言的，而`leader_epoch`是相对于分区来说的。
  - 由于各请求达到顺序不同，控制器通过`controller_epoch`和`leader_epoch`来确定具体应该执行哪个命令操作。

- 已分配副本（assigned replica）：

  - 每个分区的所有副本集合被称作已分配副本，简写为 AR，本书中所有 AR 均表示此含义，而 ISR 是与分区 Leader 保持同步的副本列表。

- LeaderAndIsr：

  - Kafka 将 Leader 对应的 brokerId 和 ISR 列表封装成一个 LeaderAndIsr类。
  - 以 JSON 串表示为`{"leader" :Leader的 brokerId, "leader_epoch":leader 更新次数, "isr" :ISR 列表}`。

- 优先副本（preferred replica）：

  - 在 AR 中，第一个副本称为 preferred replica，也就是我们说的优先副本。
  - 理想情况下，优先副本即是该分区的 Leader，Kafka 要确保所有主题的优先副本在 Kafka 集群中均衡分布，这样就保证了所有分区的 Leader 均衡分布。
  - 保证 Leader 在集群中均衡分布很重要，因为所有的读写请求都由分区 Leader 副本进行处理，如果 Leader 分布过于集中，就会造成集群负载不均衡。
  - 为了保证优先副本的均衡分布，Kafka 提供了5种分区选举器（PartitionLeaderSelector），当分区数发生变化或是分区 Leader 宕机时就会通过分区选举器及时选出分区新的 Leader。

#### 2.1 控制器初始化

- 每个代理在启动时会实例化并启动一个 KafkaController。
- KafkaController 实例化时主要完成以下工作。

 (1) 创建一个 ControllerContext 实例对象，该对象很重要的一个作用是用于缓存控制器各种处理操作所需要的数据结构。ControllerContext 实例化时会初始化用于记录控制器选举次数的 epoch 及与之对应的 zkVersion 字段的值，初始时都为0，同时设置当前正常运行的代理列表、主题列表、各主题对应分区与副本的AR列表等。声明控制器与其他代理通信的 ControllerChannelManager 对象，ControllerChannelManager 在这里只是声明并没有创建和启动。实例化代理选举控制器操作的 ReentrantLock。

 (2) 实例化用于维护和管理分区状态的状态机（PartitionStateMachine）。Kafka 分区定义了4个状态，分区状态及描述如表3-1所示。

Kafka 分区状态及描述

| 状　态　名           | 状　态　值 | 描　　述                                                     |
| -------------------- | ---------- | ------------------------------------------------------------ |
| NewPartition         | 0          | 当一个分区被创建后就处于该状态，处于该状态的分区已得到了副本分配，但还没有Leader和ISR信息 |
| OnlinePartition      | 1          | 一旦分区的Leader选举出来后，分区就处于该状态                 |
| OfflinePartition     | 2          | 如果分区Leader成功选出来之后，Leader的代理仍处于宕机状态，则该分区就转到离线（OfflinePartition）状态 |
| NonExistentPartition | 3          | 处于此状态表明该分区可能还没有被创建，也可能是曾被创建过但已被删除 |

分区状态机对应的4种状态的转换关系

分区状态机会注册两个监听器，这两个监听器的作用如下。

- TopicChangeListener 用于监听 `/brokers/topics` 路径下子节点变化，当创建一个主题时，会在该路径下创建一个与该主题相同名字的子节点。当该路径下子节点发生变化即主题有变化时就会触发该监听器，该监听器的 handleChildChange() 方法中会更新 ControllerContext 中维护的主题列表信息及各主题对应分区的 AR 信息。同时，若该路径下节点的变更是由于创建一个新的主题所引起的，则调用控制器相应方法进行处理，会向该主题注册一个用于监听该主题分区和副本发生变化的监听器。

- DeleteTopicsListener 用于监听 `/admin/delete_topics`子节点的变更，当删除一个主题时，会在该路径下创建一个与待删除主题相同名字的子节点，当该路径下子节点发生变化时就会触发该监听器，在该监听器的 handleChildChange() 方法中会将待删除的主题从 /brokers/topic 路径下删除，并将该主题加入到 TopicDeletionManager 维护的记录待删除主题的队列当中，交由 TopicDeletionManager 执行删除。

  （3）实例化一个对副本状态管理的状态机 ReplicaStateMachine。Kafka 对副本定义了7种状态，各副本状态及描述如表3-2所示。

  表3-2　Kafka 副本状态及描述

| 状　态　名                | 状态值 | 描　　述                                                     |
| ------------------------- | ------ | ------------------------------------------------------------ |
| NewReplica                | 1      | 新创建的副本状态即为 NewReplica，处于该状态的副本只能接受成为 Follower 副本的转换请求 |
| OnlineReplica             | 2      | 一旦副本被启动或者已是分区 AR 的一部分时，该副本就处于在线状态，处于该状态的副本可以接受成为 Leader 或者 Follower 的转换请求 |
| OfflineReplica            | 3      | 如果检测到一个副本所在的代理已宕机，则将该副本状态设置为此状态即离线状态，表示该副本将要被从 ISR 中下线 |
| ReplicaDeletionStarted    | 4      | 在对处于离线状态的副本进行删除操作时，先将副本状态标记为此状态，表示正在进行删除离线副本的操作 |
| ReplicaDeletionSuccessful | 5      | 当删除副本成功，删除请求返回没有错误应答码时，则将副本标记为此状态 |
| ReplicaDeletionIneligible | 6      | 如果副本删除失败，则将副本状态设置为此状态                   |
| NonExistentReplica        | 7      | 如果副本删除成功，则将副本状态设置为此状态                   |

表3-2对副本状态进行了详细介绍，各状态之间有效状态转换如图3-4所示。

![enter image description here](../../../../../../../../images/2018/10/副本状态.png)

图3-4　副本状态机状态转换图

在副本状态机内部定义了一个 BrokerChangeListener 监听器，该监听器会监听 ZooKeeper 的 /brokers/ids/ 路径下各代理对应的 brokerId 节点的变化。当代理与 ZooKeeper 会话超时，相应临时节点被删除抑或是一个新代理加入时，/brokers/ids 路径下的子节点发生变化就会触发该监听器，该监听器调用 ControllerContext.controllerChannelManager 对节点变化进行相应处理。

（4）创建用于将当前代理选举为控制器的 ZooKeeperLeaderElector 选举器对象，实例化该对象时需要传递两个回调函数：完成控制器相应初始化操作的 onControllerFailover() 方法，以及当新的控制器当选时让先前的控制器注销控制器权限的 onControllerResignation() 方法。Kafka 控制器选举策略是在 ZooKeeper 的 /controller 路径下创建一个临时节点，并注册一个 LeaderChangeListener，通过该监听器来监听该临时节点，当该临时节点信息发生变更时，就会触发该监听器。当新当选的控制器信息被保存时，就会触发该监听器的 handleDataChange() 方法进行相应处理；当监听器监听到 /controller 路径下控制器信息被删除时，将触发 onControllerResignation() 回调方法，同时触发重新选举机制。关于控制器的选举过程在后面小节会有详细讲解，这里不再赘述。

（5）创建一个独立定时任务 KafkaScheduler，该定时任务用于控制器进行平衡操作，其生命周期只在代理成为Leader控制器期间有效，当代理不再是Leader控制器时，即调用 onControllerResignation() 方法时该定时任务就会被关闭。

（6）声明一个对主题操作管理的 TopicDeletionManager 对象。该对象在3.2.5节将会详细阐述。

（7）创建一个用于在分区状态发生变化时为分区选举出 Leader 副本的分区选举器 PartitionLeaderSelector。Kafka 提供了5种分区选举器，这些选举器会被分区状态机调用，当分区的状态发生变化时，根据分区所处状态选用不同的选举器为分区选出 Leader 副本。分区选举器只定义了一个 selectLeader() 方法，该方法接受两个对象，一个表示要被选举为 Leader 的分区对象 TopicAndPartition，另一个表示该分区当前的 Leader 和 ISR 对象 LeaderAndIsr。该方法返回一个元组，元组包括新当选的 Leader，ISR 对象以及由一组副本构成的 LeaderAndIsrRequest 对象。分区选举器的类层次结构

![分区选举器的类层次结构](../../../../../../../../images/2018/10/分区选举器的类层次结构.png)

各选举器的功能及选举策略如下。

- OfflinePartitionLeaderSelector：分区状态机启动、新创建一个分区或是将一个分区状态由 NewPartition 状态、OfflinePartition 状态转换到 OnlinePartition 状态时会调用该选举器，为分区选出 Leader，得到分区的 LeaderAndIsr。该选举器选举策略是首先判断是否有存活（本书用存活来代表心跳检测正常的代理所处的状态）的 ISR，若 ISR 中至少有一个存活的代理，则从 ISR 列表中选第一个存活的代理作为 Leader，存活的 ISR 作为新的 ISR；否则，若配置项 unclean.leader.election.enable 为 true，该配置项默认为 true，即表示允许从不在 ISR 列表中的副本选举 Leader，同时 AR 中若有存活的副本，则从 AR 列表中选第一个代理作为 Leader，存活的 AR 作为新的 ISR。当没有可选作 Leader 的代理时，会抛出 NoReplicaOnlineException 的异常。Leader 和 ISR 选出后构造 LeaderAndIsr 对象，将当前的 `leader_epoch` 加1赋值给新的`leader_epoch`，将当前的 zkVersion 加1作为新的 zkVersion。若 Leader 选举成功，后续会将 LeaderAndIsr 对象和`controller_epoch`值构造 PartitionStateInfo 对象。登录 ZooKeeper 客户端在 ZooKeeper 的 `/brokers/topics/${topicName}/partitions/${partitionId}/state`可以查看到某个分区的元数据信息。例如，查看有5个 Broker，即 brokerId={0,1,2,3,4}，主题名 topicName 为“topic-analyse”，partitionId 为0的分区信息如下：

  ```json
   {"controller_epoch":7,"leader":3,"version":1,"leader_epoch":0,"isr":[1,3]}
  ```

  以上分区信息表示该分区的 Leader 副本为 brokerId 为3的节点，同时该分区至少有2个副本，因为 ISR 当前有2个节点，集群的控制器总共发生了7次变更，而分区的 Leader 从最初当选还未发生过变化，可以从`leader_epoch`值来判断分区或是ISR是否发生过变化。

- ReassignedPartitionLeaderSelector：当分区进行重分配时会调用该选举器。该选举器的选举策略是从 AR 列表中找出存活副本列表，若有存活的副本则取存活副本列表的第一个副本作为 Leader，将当前 ISR 作为新的 ISR，将 AR 作为接受 LeaderAndIsr 请求的副本集合。若没有候选的副本，则抛出 NoReplicaOnlineException 异常。

- PreferredReplicaPartitionLeaderSelector：该选举器直接将优先副本设置为分区的 Leader。该选举器首先根据当前 Leader 是不是由优先副本担任来决定是否需要选举。若当前 Leader 由优先副本担任则无需设置，仅抛出 LeaderElectionNotNeededException 异常进行提示；若优先副本不是 Leader 但在该分区的 ISR 列表中，则将优先副本选为 Leader，将 AR 作为接受 LeaderAndIsr 请求的副本集合；否则抛出 StateChangeFailedException 异常。

- ControlledShutdownLeaderSelector：该选举器将从 ISR 中剔除已关闭的节点，将剔除已关闭节点后的 ISR 作为新的 ISR，同时从新的 ISR 中选取第一个作为 Leader 副本。将 AR 中剔除已关闭节后的副本节点作为接受 LeaderAndIsr 请求的副本集合。

- NoOpLeaderSelector：该选举器只返回当前分区的 Leader 和 ISR。

  （8）实例化 ControllerBrokerRequestBatch。在前面实例化了分区状态机和副本状态机，这两个状态机在相应状态发生变化时相应监听器都会调用各自的 handleStateChange() 方法进行处理，而 ControllerBrokerRequestBatch 封装了 leaderAndIsrRequestMap、stopReplicaRequestMap 和 updateMetadataRequestMap 这3个集合，用来记录和缓存 handleStateChange() 方法中产生的 request，控制器将这些 request 交由 ControllerBrokerRequestBatch.sendRequestsToBrokers() 方法批量发送出去，交由 KafkaApis 调用相应的 handle 方法进行处理。

  （9）实例化3个监听器，即用于监听分区重分配的 PartitionsReassignedListener，用于监听当分区状态变化时触发 PreferredReplicaPartitionLeaderSelector 选举器将优先副本选举为 Leader 的 PreferredReplicaElectionListener、用于监听当 ISR 发生变化时将 ISR 变化通知给 ZooKeeper 进行更新操作，同时向所有的代理节点发送元数据修改请求的 IsrChangeNotificationListener。

  至此，控制器实例化过程讲解完毕。当一个代理启动时就会创建一个 KafkaController 实例并启动。在启动 KafkaController 时，先注册一个用于监听代理与 ZooKeeper 会话超时的监听器 SessionExpirationListener，然后启动控制器选举，让当前代理试图去竞选为控制器。

以下部分实在目前消化困难，待以后回头温习，顾直接引用。

#### 2.2 控制器选举过程

每个代理启动时会创建一个 KafkaController 实例，当 KafkaController 启动后就会从所有代理中选择一个代理作为控制器，控制器是所有代理的 Leader，因此这里也称之为 Leader 选举。除了在启动时会导致选举外，当控制器所在代理发生故障或 ZooKeeper 通过心跳机制感知控制器与自己的连接 Session 已过期时，也会再次从所有代理中选出一个节点作为集群的控制器。

Kafka 控制器的选举依赖于 ZooKeeper。在集群整个运行过程中，代理在 ZooKeeper 不同节点上注册相应的监听器。各监听器各司其职，当所监听的节点状态发生变化时就会触发相关函数进行处理。本节将详细讲解控制器的选举过程。

在3.2.1节中提到过在控制器初始化时创建了一个将代理选举为 Leader 的 ZooKeeperLeaderElector 对象，代码如下：

```
val controllerElector=new ZooKeeperLeaderElector(controllerContext, ZkUtils.ControllerPath,   
onControllerFailover, onControllerResignation, config.brokerId)
```

通过实例化 ZooKeeperLeaderElector 的入参可看到选举前后需要做的工作。参数 config.brokerId 是候选控制器代理对应的 brokerId，参数 ZkUtils.ControllerPath 即为选举过程所依赖的 /controller 路径，当代理当选为 Leader 控制器后回调 onControllerFailover，当前代理上任（sign），注册 Leader 拥有的权限和启动相应工作；当代理不再担任 Leader 时，当前代理退位（resign），注销 Leader 拥有的权限，即回调 onControllerResignation 相应进行处理。

在 ZooKeeperLeaderElector 启动时首先注册一个 LeaderChangeListener，负责监听 ZooKeeper 的 /controller 节点数据变化，该节点存储了当前 Leader 的 brokerId，数据格式为一个 JSON 字符串：{“version” :1, “brokerid”:brokerId, “timestamp”:timestamp}。当该节点数据发生变化时，比较当前代理的 brokerId 与当前 Leader 的 leaderId 是否相同，若不同，则表示当前代理已不是 Leader，则回调 onControllerResignation 退位，注销 Leader 控制器相关权限，将当前代理状态设置为 RunningAsBroker，同时将该代理的`leader_epoch`和 zkVersion 设置为0。当该节点数据被删除时，若当前代理是 Leader，则先退位，然后再触发选举，否则直接触发选举。

控制器选举算法思想较简单，入口为 ZooKeeperLeaderElector.select() 方法，该方法执行逻辑如下。

每个代理首先从 ZooKeeper 的 /controller 节点获取 Leader 信息，解析当前 Leader 的 leaderId（作为 Leader 的代理对应的 brokerId）。若 leaderId 等于−1，则表示还没有成功选举出 Leader，则该代理将封装有自己 brokerId 的信息以 JSON 串 {“version”: 1, “brokerid”: brokerId, “timestamp”:timestamp} 格式请求 ZooKeeper 将该数据写入 /controller 节点。如果 leaderId 不为−1，则表示已有代理抢先成为了 Leader，则停止选举。若写入代理信息成功，则当前代理即为所选出的 Leader。

在抢占写 /controller 节点时若发生非 ZkNodeExistsException 异常，则会将 leaderId 设置为−1，同时删除存储在 /controller 节点的元数据信息，以便让请求最先到达 ZooKeeper 的代理成为 Leader，由于删除了 /controller 节点将会触发 LeaderChangeListener.handleDataDeleted() 方法，就会重新选举 Leader。同时由于 /controller 节点数据的变化，将触发 LeaderChangeListener.handleDataChange() 方法，这时其他代理将通过当前的 leaderId 与自己的 brokerId 比较，若在 /controller 节点数据发生变化前自己是 Leader，而现在 leaderId 与自己的 brokerId 不同，则自己退位（resign），回调 onControllerResignation 函数。

可见，Kafka 控制器选举的核心思想就是各代理通过争抢向 /controller 节点请求写入自身的信息，先成功写入的代理即为 Leader。控制器选举流程如图3-6所示。

![enter image description here](../../../../../../../../../../../../images/2018/10/控制器选举流程.png)

图3-6　控制器选举流程

#### 2.3　故障转移

我们在3.2.2节对选举过程进行了详细介绍，而触发控制器进行选举有3种情况：一是在控制器启动的时候，二是当控制器发生故障转移的时候，三是当心跳检测超时的时候。因此，我们说控制器故障转移的本质是控制权的转移，而控制权的转移也就是重新选出新的控制器。在控制器实例化时创建了一个 ZooKeeperLeaderElector 对象，实例化该对象时需要两个回调函数，分别用于代理当选为控制器时注册相应权限的 onControllerFailover() 方法和不再是 Leader 控制器时注销相应权限的 onControllerResignation() 方法。对故障转移的讲解，我们也是主要介绍这两个方法具体的实现逻辑。

##### 1．onControllerFailover 操作

KafkaController 的 onControllerFailover() 方法的作用就是完成控制器相应的初始化工作，如果当前的控制器正常运行，即在控制器启动时的标志位 isRunning 为 true，则执行以下逻辑完成控制器的初始化，否则表示当前的控制器已关闭，将终止相应的初始化处理。

（1）从 ZooKeeper 的 `/controller_epoch` 路径读取当前控制器的轮值次数，并更新到当前 ControllerContext 中。

（2）将控制器的轮值次数加1，并尝试去更新 ZooKeeper 中`/controller_epoch`中记录的轮值次数的值，若更新失败则表示当前的控制器也被其他控制器替代，因此当前代理成为控制器相关的初始化处理将以异常而告终。若更新失败是由于 ZooKeeper 中不存在 `/controller_epoch` 节点，则表明是控制器首次启动，第一个控制器当选，因此在 ZooKeeper 中创建该节点并写入控制器轮值次数。同时更新 ControllerContext 中缓存的与轮值次数相关的数据。

前两步是判断能否为控制器完成初始化处理的前置条件，只有保证控制器轮值次数正常处理之后，才会进行以下初始处理工作。

（1）注册分区管理相关的监听器。用于监听 ZooKeeper 的`/admin/reassign_partitions`节点引发分区重分配操作的 PartitionsReassignedListener；用于监听 ZooKeeper 的`/isr_change_notification`节点用于处理分区ISR发生变化的 IsrChangeNotificationListener；用于监听`/admin/preferred_replica_election`节点将优先副本选为分区 Leader 操作的 PreferredReplicaElectionListener。

（2）注册主题管理的监听器。通过分区状态机向 ZooKeeper 的 /brokers/topics 节点注册一个 TopicChangeListener，用于监听主题发生变化时进行相应的处理。同时，若开启了 delete.topic.enable，即该配置项值为 true，则同时向 ZooKeeper 的 `/admin/delete_topics`节点注册一个 DeleteTopicsListener，该监听器会完成服务器端删除主题相应的操作，否则当客户端删除一个主题时，仅是将该主题标识为删除，但服务端并没有将该主题真正删除。

（3）注册代理变化处理的监听器。通过副本状态机向 ZooKeeper 的 /brokers/ids 节点注册一个 BrokerChangeListener，当代理发生增、减变化时进行相应的处理。

（4）初始化 ControllerContext，即当一个代理成为控制器后，原控制器所持有的 ControllerContext 将被重新赋值。首先从 ZooKeeper 中获取当前所有存活的代理、所有的主题及分区分配方案信息，分别初始化存活的代理集合、主题集合及每个分区的AR集合信息，更新 ControllerContext 中每个分区的 Leader 及 ISR 信息。当 ControllerContext 缓存的基础信息初始化后，启动用于管理控制器与各代理之间通信的 ControllerChannelManager。然后分别初始化需要优先副本选举的分区，以及需要选举的分区所对应的分区分配方案。最后创建一个用于管理主题删除操作的 TopicDeletionManager 对象。

（5）启动分区状态机和副本状态机。

（6）从 ControllerContext 中读取所有主题，轮询每个主题，为每个主题添加用于监听分区变化的 PartitionModificationsListener。

（7）检测当前是否有分区需要触发分区重分配操作。若需要重分配，则进行一次分区重分配操作。

（8）检测当前是否有需要将优先副本选举为 Leader 的分区，并进行相应的操作。

（9）向 Kafka 集群中所有存活的代理发送更新元数据请求。

（10）根据配置 auto.leader.rebalance.enable 决定是否创建用于分区平衡操作的定时任务。该配置项默认为 true。若该配置项为 true，则创建一个每隔`${leader.imbalance.check.interval.seconds}`秒，默认是300秒，即每5分钟执行一次分区重分配检查及分区重分配操作的定时任务。

（11）启动第4步创建的删除主题管理的 TopicDeletionManager 组件。

至此，onControllerFailover() 操作的执行逻辑介绍完毕。通过该方法执行逻辑讲解可知，当一个代理成为控制器后，主要完成相应元数据的初始化以及对代理、主题、分区等变化感知的监听器的注册和启动相应管理组件。

##### 2．onControllerResignation 操作

当一个代理不再是控制器时，需要注销控制器相应的权限及修改相应元数据的初始化信息。KafkaController 通过调用 KafkaController.onControllerResignation() 方法实现一个代理从控制器到普通代理的转变操作，该方法的执行逻辑如下。

首先，取消该控制器在 ZooKeeper 中注册的用于对分区及副本变化感知的监听器的监听；接着，关闭删除主题操作的 TopicDeletionManager，并关闭分区平衡操作的定时任务（若参数 auto.leader.rebalance.enable 为 true，即在当前代理成为控制器时启动的分区平衡操作的定时任务）。

然后，在获取 ControllerContext 维护的重入锁的条件下取消对分区 ISR 变化监听，关闭分区状态机和副本状态机，关闭控制器与其他代理之间进行通信的 ControllerChannelManager。

最后，将 ControllerContext 中用于记录控制器轮值次数及轮值数对应的 epochZkVersion 字段置零，并将当前代理状态设置为 RunningAsBroker，即当前代理不再是控制器的角色。

#### 2.4　代理上线与下线

在介绍完控制器选举操作两个回调方法之后，我们再简要对代理增、减变化，或者说是代理上线与下线操作时 BrokerChangeListener 所做的处理进行介绍。

##### 1．代理上线

当有新的代理上线时，在代理启动时会向 ZooKeeper 的 /brokers/ids 节点下注册该代理的 brokerId，此时会被副本状态机在 ZooKeeper 所注册的 BrokerChangerListener 监听器监听到该节点信息的变化，通过 ZooKeeper 中记录的节点信息及 ControllerContext 缓存的节点信息，计算出新上线的节点集合，对新上线的代理节点调用 ControllerChannelManager.addBroker() 方法完成新上线代理网络层相关初始化处理。然后调用 KafkaController.onBrokerStartup() 方法进行处理，该方法处理逻辑如下。

首先，向集群当前所有的代理发送 UpdateMetadataRequest 请求，这样所有的代理通过这种方式就会知道有新的代理加入。

接着，查找出被分配到新上线节点上的副本集合，通过副本状态机对副本状态进行相应变迁处理，将这些副本的状态更新为 OnlineReplica，并通过分区状态机对分区状态为 NewPartition 和 OfflinePartition 的分区进行处理，将其状态扭转至 OnlinePartition 状态，并触发一次分区 Leader 选举，以确认新增加的代理是否是分区的 Leader。

然后，轮询被分配到新上线代理的副本，调用 KafkaController.onPartitionReassignment() 方法执行服务端分区副本分配操作。在3.2.6节的“分区重分配”小节将对 onPartitionReassignment() 方法进行详细讲解。

最后，恢复由于新代理上线而被暂停的删除主题操作的线程，让其继续完成服务端删除主题的操作。

至此，新代理上线时 BrokerChangeListener 所进行的处理基本流程介绍完毕。当然，每步操作都涉及更复杂数据结构的处理，这里我们只是简要梳理了对代理上线操作的几个关键步骤，这样可以便于大家对新代理上线处理整体逻辑的把握，同时为阅读源码提供参考。

##### 2．代理下线

当代理下线时，该代理在 ZooKeeper 的 /brokers/ids 节点注册的与该代理对应的节点将被删除，此时 BrokerChangeListener 的 handleChildChange() 方法将被触发。

与新代理上线操作类似，首先要查找下线节点的集合，然后轮询下线节点集合，调用 ControllerChannelManager.removeBroker() 方法，关闭每个下线节点的网络连接，清空下线节点的消息队列，关闭下线节点发送 Request 请求的线程等。最后调用 KafkaController 的 onBrokerFailure() 方法进行处理，该方法处理逻辑如下。

首先，查找 Leader 副本在下线节点上的分区，将这些分区的状态设置为 OfflinePartition，并处理相应状态变迁，然后调用分区状态机的 triggerOnlinePartitionStateChange() 方法将处于 OfflinePartition 状态的分区状态转换为 OnlinePartition 状态，这个过程会通过 OfflinePartitionLeaderSelector 分区选举器为分区选出 Leader，并将 Leader 和 ISR 信息写入 ZooKeeper 中，同时发送 UpdateMetadataRequest 请求更新元数据信息。

然后，查找所有在下线节点上的副本集合，将该集合分成两部分，一部分是待删除主题的副本，将这些副本的状态转换为 ReplicaDeletionIneligible，标记该副本对应的主题暂时不可被删除。另一部分副本即是当前处于正常使用的主题的副本，因此需要对这些副本进行下线相应的处理，将副本状态由 OnlineReplica 转化为 OfflineReplica，此时会将该副本节点从分区的 ISR 集合中删除，并发送 StopReplicaRequest 请求，停止该副本从 Leader 副本同步消息的操作，发送 LeaderAndIsrRequest 请求，该分区 Leader 副本和 Follower 副本根据角色不同分别进行相应处理，同时发送 UpdateMetadataRequest 请求，更新当前所有存活代理的缓存的元数据信息。

最后，若分区 Leader 副本分配在下线节点上的所有分区状态转换操作执行完成，则向集群所有存活的代理发送更新元数据的 UpdateMetadataRequest 请求，执行元数据更新操作。

#### 2.5　主题管理

在3.2节一开始就提到控制器负责对主题、分区副本的管理操作，本小节将详细介绍控制器是如何对主题进行管理的，主要是讲解控制器在创建主题与删除主题时承担的职责。由于分区、副本是主题的固有属性，因此在讲解控制器对主题管理时将同时讲解控制器对分区副本创建及删除的管理操作。控制器对分区、副本的管理在逻辑上体现在分区状态机以及副本状态机对 ZooKeeper 的 /brokers/topics 节点及其子节点注册的一系列监听器上。

为了便于理解，我们首先创建一个主题名为“topic-foo”的主题，该主题有3个分区、2个副本，然后以该主题为例详细分析控制器在该主题创建及删除操作时具体执行逻辑。

##### 1．创建主题

当创建一个主题时会在 ZooKeeper 的 /brokers/topics 目录下创建一个与主题同名的节点，在该节点下会记录该主题的分区副本分配方案。关于主题在 ZooKeeper 创建节点的流程本小节不进行阐述，在4.2节再进行详细讲解，这里关注的是控制器在主题创建及删除操作时所承担的工作。

当创建一个主题时，无论是通过 Kafka API 还是通过命令行创建主题，同步返回创建主题成功时，其实仅是在 ZooKeeper 的 /brokers/topics 节点成功创建了该主题对应的子节点。而服务端创建主题的相关操作是异步交由控制器去完成的。例如，本例仅是在 ZooKeeper 的 /brokers/topics 路径下创建一个名为“topic-foo”的节点，同时在该节点中写入分区副本分配信息。可以登录 ZooKeeper 客户端通过 get/brokers/topics/topic-foo 命令查看该主题各分区副本分配信息，该主题各分区副本分配信息为：

```
{"version":1,"partitions":{"2":[3,2],"1":[2,1],"0":[1,3]}}
```

控制器初始化时分别创建了分区状态机及副本状态机，当代理被选为控制器后回调 onBecomingLeader() 时会调用分区状态机和副本状态机的 registerListeners() 方法。分区状态机在该方法中注册一个监听 ZooKeeper 的 /brokers/topics 子节点的变化的 TopicChangeListener 监听器，即该监听器用于监听主题及分区变化，而副本状态机在 registerListeners() 方法中会注册一个 BrokerChangeListener 监听器，该监听器用于监听 /brokers/ids 子节点的变化。当创建一个主题时，主题及该主题分区副本分配方案写入 ZooKeeper 的 /brokers/topics/ 下相应节点时，分区状态机和副本状态机注册的监听器就会被触发。

当新创建一个主题时，该主题及分区副本分配信息写入 /brokes/topics 路径下后就会触发 TopicChangeListener 监听器的 handleChildChange() 方法进行处理，在 ControllerContext 实例化时创建了一个 ReentrantLock 锁对象，handleChildChange() 方法是在获取该重入锁的条件下进行处理的。

handleChildChange() 方法的具体逻辑如下。

（1）获取 /brokers/topics 下所有主题列表集合，记为集合 A。将 ControllerContext 中记录的当前所有主题列表集合，记为集合 C。通过集合 A 与集合 C 的差集 A−C，计算出新创建的主题列表，记为集合 N，对本例而言，即新创建的主题列表中只有“topic-foo”这一个元素。通过集合 C−A 的差集计算出被删除的主题列表，记为集合 D，对本例而言 D 为一个空集合。

（2）更新 ControllerContext 缓存的当前主题列表，即用 A 集合的值覆盖 C 集合的值，这样就保证新创建的主题加入到缓存当中，同时从缓存中剔除被删除的主题。

（3）遍历集合 N 列表，读取集合中每个主题的 partitions 子节点，本例为读取 /brokers/topics/ topic-foo/partitons 下各分区副本分配方案信息，构造一个 Map[TopicAndPartition, Seq[Int]] 集合，以每个主题的每个分区 TopicAndPartition 对象作为 Key，以该分区的 AR 作为 Value。同时从分区副本分配信息中过滤掉集合 N 中所有主题对应的分区副本分配信息，然后更新缓存中分区副本分配信息。

（4）待缓存中主题及分区副本信息更新后，调用控制器的 onNewTopicCreation() 方法，实现真正创建主题的逻辑。这里之所以称为“真正创建主题”，是因为截止到当前所有的操作仅是在 ZooKeeper 中创建主题和分区副本最基础的元数据信息，以及 ControllerContext 缓存的信息，并不涉及分区及副本的状态转换、分区 Leader 分配、分区存储日志文件的创建等。这一系列的操作是由控制器调用 KafkaController.onNewTopicCreation() 方法来完成的。

第4步中的 KafkaController.onNewTopicCreation() 方法的实现逻辑如下。

（1）遍历集合 N，通过分区状态机为每个新创建的主题向 ZooKeeper 注册一个监听分区变化的监听器 PartitionModificationsListener。对本例而言，该监听器监听的是 /brokers/topics/topic-foo/ partitions 节点的信息变化。

（2）调用控制器的 onNewPartitionCreation() 方法创建分区。

上面提到的 onNewPartitionCreation() 方法的处理逻辑如下。

（1）调用分区状态机的 handleStateChanges() 方法，将新增主题的各分区状态设置为 NewPartition 状态。

（2）调用副本状态机的 handleStateChange() 方法，将新增主题的每个分区的副本状态设置为 NewReplica 状态。

（3）调用分区状态机的 handleStateChanges() 方法，将新增主题的各分区状态设置为 OnlinePartition 状态，将各分区 AR 中第一个副本选为该分区的 Leader，将 AR 作为 ISR，然后创建各分区节点并写入该分区的详细元数据信息。本例会在 /brokers/topics/topic-foo 路径分别创建3个分区元数据信息对应的节点，例如，对分区编号为1的分区元数据路径为 /brokers/topics/ topic-foo/partitions/1/state，将该分区的元数据信息写入到该节点下，同时更新 ControllerContext 中缓存的信息。登录 ZooKeeper 客户端通过 get 命令查看该分区元数据信息为：

```
{"controller_epoch":31,"leader":2,"version":1,"leader_epoch":0,"isr":[2,1]}
```

（4）调用副本状态机将新增主题的各分区的副本状态从 NewReplica 转换为 OnlineReplica 状态。

以上每步操作时都会向各代理发出请求，调用 ControllerChannelManager.sendRequestsToBrokers() 方法，在该方法中会向代理发送 LeaderAndIsrRequest 和 UpdateMetadataRequest 请求，这两类请求分别由 KafkaApis 的 handleLeaderAndIsrRequest() 方法和 handleUpdateMetadataRequest() 方法处理，前者会根据各副本是 Leader 还是 Follower 进行相应处理，后者使代理及时更新各自缓存的元数据信息以达到信息同步。至此，在新创建一个主题时，控制器对主题创建过程的管理逻辑讲解完毕。

##### 2．删除主题

客户端执行删除主题操作时仅是在 ZooKeeper 的`/admin/delete_topics`路径下创建一个与待删除主题同名的节点，返回该主题被标记为删除，保证本步操作成功执行的前提是配置项 delete.topic.enable 值被设置为 true。例如，删除主题“topic-foo”，则客户端执行删除操作时会在`/admin/delete_topics`路径下创建一个名为“topic-foo”的节点。而实际删除主题的逻辑是异步交由 Kafka 控制器负责执行的，本小节将介绍控制器在删除主题时的具体实现。

在控制器实例化时创建了一个分区状态机，而分区状态机注册了一个监听 ZooKeeper 的`/admin/delete_topics`子节点变化的监听器，即 DeleteTopicsListener 监听器。当客户端执行删除主题操作将待删除主题写入`/admin/delete_topics`路径下时，将会触发该监听器。在该监听器的 handleChildChange() 方法中执行实际删除主题操作。

handleChildChange() 方法在获取 ControllerContext 的 ReentrantLock 的条件下执行，具体执行逻辑如下。

（1）从 `/admin/delete_topics` 路径下读取被标记为删除的主题，记为集合 D，从 ControllerContext 中读取缓存的所有主题列表，记为集合 A。通过计算集合 D 与集合 A 的差集得到已被删除的主题集合，记为 N。由于实际删除主题是由控制器负责执行的，而可能在删除某些主题相应的元数据之后相应的缓存数据已更新，但在删除`/admin/delete_topics`路径该主题节点之前发生了异常或未被执行，如控制器的平衡操作控制权发生了转移等，这样就会导致某些该主题对应的节点还存在`/admin/delete_topics`路径下。

（2）若集合 N 不为空，则直接删除`/admin/delete_tpoics`路径下集合 N 中各主题对应的节点。

（3）计算集合 D 与集合 N 的差集，计算本次操作实际应删除的主题集合，得到新的待删除集合 D。若集合 D 为空，则本次删除操作结束，否则转入第4步执行。

（4）遍历集合 D 中的各主题，分别检测主题当前是在进行将优先副本选为分区 Leader 操作还是在进行分区重分配操作。若是这两种操作中的一种，则将该主题加入到暂时不可删除的主题集合中缓存起来，等分区 Leader 选举结束或分区重分配结束之后，再调用 TopicDeletionManager. resumeDeletionForTopics() 方法将这些主题从不可删除的主题集合中剔除，同时唤醒删除主题的线程 DeleteTopicsThread 进行删除操作；否则，调用 TopicDeletionManger.enqueueTopicsForDeletion() 方法来唤醒删除主题线程，执行删除主题操作。

（5）当第4步执行完成后，待删除主题的所有副本状态均被设置为 ReplicaDeletionSuccessful 状态，当 DeleteTopicsThread 线程检测到副本状态都为 ReplicaDeletionSuccessful 状态时将会移除状态机对该主题分区的监听器，同时将该主题对应的副本状态设置为 NonExistentReplica，然后分别删除该主题在 ZooKeeper 存储元数据对应的节点。对于本例而言，会依次删除主题“topic-foo”在 ZooKeeper 的 /brokers/topics/topic-foo 节点、/config/topics/topic-foo 节点以及 `/admin/delete_topics/topic-foo` 节点。最后将该主题对应的缓存信息移除。

补充说明一下，第4步中提及的 DeleteTopicsThread 线程的执行逻辑如下：该方法首先对待删除主题的所有副本状态进行检测并进行相应处理，在这里我们只分析该主题所有副本都处于可被执行删除操作的情况，此时删除主题线程调用 onTopicDeletion() 方法执行删除操作。

onTopicDeletion() 方法的执行逻辑如下。

（1）向所有代理发送更新元数据的请求，即发送 UpdateMetadataRequest，通知所有代理将待删除的主题分区副本信息从缓存中删除。

（2）调用 TopicDeletionManager.onPartitionDeletion() 方法执行删除该主题的所有分区操作，而该方法主要完成以下逻辑操作。

（a）向所有代理发送请求，通知代理该主题的所有分区将要被删除，代理收到请求后就会拒绝客户端向这些分区发送的请求。

（b）通过副本状态机将该主题每个分区的所有副本状态设置为 OfflineReplica 状态，在副本状态设置为 OfflineReplica 过程当中会向所有副本对应的代理发送 StopReplicaRequest 请求，下达不再向该主题的分区 Leader 副本发送请求的指令，这样该分区的 ISR 就不断缩小，当该分区 Leader 对应的副本状态也被置于 OfflineReplica 状态时，该分区 Leader 信息将被更新为 −1，这样就不会再发送 LeaderAndIsrRequest 的请求，即不会向各副本节点发送副本同步的请求。

（c）通过副本状态机将该主题各副本状态设置为 ReplicaDeletionStarted 状态。此时会向该主题副本对应的所有代理发送 StopReplicaRequest 的请求，并附带删除标志位，所有代理在接收到请求后，ReplicaFetcherManager 会停止对该副本对应分区 Fetcher 线程，同时删除该副本存储数据的目录文件。

#### 2.6　分区管理

- Kafka 控制器对分区的管理包括对分区创建及删除的管理，分区 Leader 选举的管理，分区自动平衡、分区副本重分配的管理等。

##### 1．分区平衡

- 在 onControllerFailover 操作时会启动一个分区自动平衡的定时任务，该定时任务会定期检查集群上各代理分区分布是否失去平衡。
  - 该过程是调用控制器的 `checkAndTriggerPartitionRebalance()` 方法完成。
- 分区自动平衡是通过将分区的优先副本选为分区的 Leader，通常当分区副本是通过 Kafka 自动分配时，会保证分区的副本分配在不同的代理节点，而分区副本分配方案即AR中的第一个副本优先副本会作为该分区的 Leader。
  - 这样当每个分区的 Leader 为各自的优先副本时，Kafka 各分区就处于一个相对平衡的状态。
  - 然而，随着时间的推移，Kafka 在运行时可能部分节点的变化导致 Leader 进行了重新选举，若优先副本在发生故障后由其他副本代替担任了 Leader，就算优先副本故障解除，重新回到集群时若没有自动平衡功能，该副本也不会成为分区的 Leader。
- 下面详细讲解自动平衡的过程。
  - 首先，从 ControllerContext 的 partitionReplicaAssignment 数据结构中查询出当前所有可用的副本（剔除待删除主题的副本），根据分区 AR 的头节点分组。
  - 然后，轮询所有代理节点，以判断该节点的分区是否需要进行优先副本选举。
    - 判断的条件是计算每个代理的分区不平衡率 imbalanceRatio 是否超过了 leader.imbalance.per.broker.percentage 配置的比率，默认是10%。
    - 不平衡率是指每个代理上的分区 Leader 不是优先副本的分区总数 totalTopicPartitionsNotLedByBroker 与该代理上分区总数的比值。
    - 若不平衡率超过了`${leader. imbalance.per.broker.percentage}/100`，且没有分区正在进行分区重分配和优先副本选举操作以及当前没有执行删除主题操作，则调用 onPreferredReplicaElection() 方法，执行优先副本选举，让优先副本成为分区的 Leader，这样就达到了分区自动平衡的目的。

##### 2．分区重分配

本小节将通过对一个名为“partition-reassign-foo”主题的分区进行重新分配的实例，详细讲解分区重分配的实现原理。该主题当前的分区副本信息如图3-7所示。

![enter image description here](../../../../../../../../images/2018/10/分区副本信息.png)

图3-7 “partition-reassign-foo”分区副本分配方案

将该主题分区重分配操作之前各分区副本分配信息描述如表3-3所示，分区重分配操作各分区副本分配方案信息如表3-4所示。

表3-3　主题“partition-reassign-foo”分区重分配前副本信息

| 分区编号 | Leader | AR    |
| -------- | ------ | ----- |
| 0        | 3      | [3,1] |
| 1        | 1      | [1,3] |

表3-4　主题“partition-reassign-foo”分区重分配分配方案

| 分区编号 | Leader | AR    |
| -------- | ------ | ----- |
| 0        | 2      | [2,3] |
| 1        | 1      | [1,2] |

- 当客户端执行分区重分配操作后（客户端分区重分配相关操作在5.6.2节有详细介绍），会在 ZooKeeper 的 /admin 节点下创建一个临时子节点`reassign_partitions`，将分区副本重分配的分配方案写入该节点中。
- 由于正常情况下，分区重分配的整个操作执行过程很快，所以大多时候当我们还没来得及在 ZooKeeper 上查看到该临时节点时，由于分区重分配操作完成，该节点已被删除了。
- 如果读者希望确认在分区重分配执行过程中曾创建过该临时节点，可以查看`${KAFKA_HOME}/ logs/controller.log`日志文件，例如，本节实例在执行时日志文件会输出以下内容：

```verilog
DEBUG [PartitionsReassignedListener on 1]: Partitions reassigned listener fired for path/admin/reassign_partitions. Record partitions to be reassigned {"version":1, "partitions":[{"topic":"partition-reassign-foo","partition":0,"replicas":[2,3]},{"topic":"partition-reassign-foo","partition":1,"replicas":[1,2]}]}
```

同时还可以在执行分区重分配操作之前关闭新分配方案中的某个节点，如这里在执行时关掉了 server-2 节点。通过该方式，我们可以在 ZooKeeper 中查看整个重分配过程当中相应节点数据的变化。

在客户端执行分区重分配操作后，登录 ZooKeeper 客户端查看在`/admin/reassign_partitions`节点中写入的分区副本分配信息如图3-8所示。

![enter image description here](../../../../../../../../images/2018/10/分区副本分配信息.png)

图3-8　分区重分配时 ZooKeeper 中相应节点元数据信息

由于 `/admin/reassign_partitions`节点数据发生了变化，此时会触发 PartitionsReassignedListener 监听器，在该监听器的 handleDataChange() 方法对该主题的每个需要重新分配的分区经过一系列的检测验证处理后，最终会调用 KafkaController.onPartitionReassignment() 方法完成分区重分配的操作。该方法执行逻辑如下。

（1）由于重新分配的副本新节点肯定不在 ISR 之中，因此首先需要计算出不在 ISR 之中的副本列表。我们将新分配的分区的 AR 记为 RAR，将该主题分区重分配之前的 AR 记为 OAR，本例两个分区的 OAR、RAR 信息以及两者的差集与并集信息如表3-5所示。

表3-5　主题“partition-reassign-foo”分区的 AR 信息

| 分区编号 | OAR   | RAR   | RAR-OAR | RAR+OAR |
| -------- | ----- | ----- | ------- | ------- |
| 0        | [3,1] | [2,3] | [2]     | [2,3,1] |
| 1        | [1,3] | [1,2] | [2]     | [2,1,3] |

用 OAR 与 RAR 的并集更新该主题的分区 AR，即将并集结果写入 /brokers/topics/partition- reassign-foo 节点中，在 ZooKeeper 中的结果如图3-9所示。为了保证分区重分配的操作顺利完成，登录 ZooKeeper 客户端查看相应元数据信息之后，重启 server-2。

（2）向 AR 并集的各节点发送 LeaderAndIsrRequest 的请求，让各副本节点进行数据同步，更新缓存中分区副本信息，更新后的缓存中记录的分区信息为 OAR 与 RAR 的并集，例如，对编号为0的分区，对应的分区副本信息为[2,3,1]。并强制让`leader_epoch`数据增1。

（3）将 AR 差集的各副本的状态设置为 NewReplica。在第1步操作结束后由于各分区副本信息发生了变化，会触发 ReassignedPartitionsIsrChangeListener 监听器，该监听器也会调用 onPartitionReassignment() 方法。若此时 RAR 中的所有副本都加入到各分区的 ISR 之中，则转至该函数的第4步继续执行。

（4）通过 (OAR+RAR)−RAR 计算每个分区需要被下线的副本。本例编号为0的分区需要下线的副本为[2,3,1]−[2,3]=[1]。

（5）遍历 RAR 中各副本，调用副本状态机创建相应的副本操作，并将副本状态设置为 OnlineReplica 状态。

（6）更新缓存中 AR 信息，即用 RAR 覆盖缓存中的 AR 信息。

（7）检测各分区 Leader 并分别进行相应处理。若 Leader 不在 RAR 之中或 Leader 节点已宕机，则从 RAR 中选出一个副本作为该分区 Leader，否则向各副本发送 LeaderAndIsr 请求，并更新 ZooKeeper 中该分区的`leader_epoch`值。

（8）将需要下线的副本进行下线处理。首先将下线的副本状态设置为 OfflineReplica 状态，这样控制器就会将该副本从分区的 ISR 中移除，接着再执行将副本删除的处理。

（9）更新各分区的 AR 信息，即将该分区原来的 AR 替换为 RAR 信息。

（10）删除 ZooKeeper 中的 `/admin/reassign_partitions`临时节点。

（11）向所有的代理发送更新元数据信息请求。

（12）唤醒删除主题的线程，有可能执行了删除当前正在进行分区重分配的主题，但由于分区重分配而导致删除主题线程被挂起，因此待分区重分配完成后，再唤醒该线程，继续执行删除主题的操作。

至此，分区重分配实现原理讲解完毕。

