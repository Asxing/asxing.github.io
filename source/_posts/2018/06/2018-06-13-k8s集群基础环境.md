---
title: k8s集群基础环境
tags: [k8s, CentOS7, Docker]
img: https://www.holddie.com/img/20200105160754.jpg
date: 2018-06-13 22:01:24
categories: k8s
---

拿起是实力，放下是智慧。															——孤灯星使



经历了两天的摸索，对于服务的改造，从最初的设想从原先的openshift环境，改造开始，到最后的环境重新部署，其中遇见的一些点滴。

### 探索期

```shell
$ yum remove origin-node
$ yum remove origin-master

centos-openshift-origin

$ /usr/bin/openshift start master api --config=/etc/origin/master/master-config.yaml --loglevel=2 --listen=https://0.0.0.0:8443 --master=https://master3:8443

$ /usr/bin/openshift start master controllers --config=/etc/origin/master/master-config.yaml --loglevel=2 --listen=https://0.0.0.0:8444

$ rm -rf /usr/bin/openshift*

$ rm -rf /etc/selinux/targeted/active/modules/100/openshift

$ find / -name docker | awk '{print $1}' | xargs rm -rf 

$ find / -name openshift | awk '{print $1}' | xargs rm -rf 

$ yum install docker

$ systemctl start docker.service
```

自从卸载完docker以后出现启动异常问题：

```shell
service docker start
```

你需要：

- 编辑  `vim /etc/sysconfig/docker`  
- 然后  `OPTIONS='--selinux-enabled --log-driver=journald --registry-mirror=http://xxxx.m.daocloud.io'  registry-mirror 输入你的镜像地址`   
- 最后 service docker restart 重启 daemon 



错误一

```verilog
Error starting daemon: SELinux is not supported with the overlay2 graph ...false)
```

解决办法：

```shell
OPTIONS='--selinux-enabled=false --log-driver=journald --signature-verification=false'
if [ -z "${DOCKER_CERT_PATH}" ]; then
    DOCKER_CERT_PATH=/etc/docker
fi
```

配置中心

```properties
192.168.1.95 k8s-node-95
192.168.1.96 k8s-node-96
192.168.1.97 k8s-node-97
192.168.1.98 k8s-node-98
192.168.1.99 k8s-node-99
192.168.1.100 k8s-node-100
192.168.1.101 k8s-node-101
192.168.1.102 k8s-node-102
192.168.1.103 k8s-node-103
```

### 懵懂期

#### 安装centos7

- 使用rufus工具

- 插入优盘进行安装
  - 因为使用的是ThinkPad，所以安装过程中，直接按f12，进行选择

#### 软件安装

首先联网处理，其次修改网卡，启动网络，重启服务

修改阿里云镜像（可以不需要，默认是华为云）

```shell
$ ip addr

$ yum install net-tools
```

安装网络工具，修改网络DNS解析，绑定网卡

```shell
BOOTPROTO=static
ONBOOT=yes
IPADDR=192.168.1.102
NETMASK=255.255.255.0
NM_CONTROLLED=no
GATEWAY=192.168.1.1
DNS1=114.114.114.114
```

修改网络昵称

```shell
$ hostnamectl set-hostname k8s-node-xx
```

### 操作期

```shell
# 安装依赖包
yum install -y yum-utils device-mapper-persistent-data lvm2

# 添加Docker软件包源
yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo

#关闭测试版本list（只显示稳定版）
sudo yum-config-manager --enable docker-ce-edge
sudo yum-config-manager --enable docker-ce-test

# 更新yum包索引
yum makecache fast

#NO.1 直接安装Docker CE （will always install the highest  possible version，可能不符合你的需求）
yum install docker-ce

#NO.2 指定版本安装
yum list docker-ce --showduplicates|sort -r  
yum install docker-ce-18.03.1.ce -y
yum install docker-ce-17.03.2.ce -y
17.03.2.ce-1.el7.centos

# 启动
systemctl start docker
# 测试
docker run hello-world
docker version
# 卸载
yum remove docker-ce
rm -rf /var/lib/docker
```

### 安装环境准备

- 关闭防火墙，关闭 `SELinux`
- 配置结点解析



安装环境：

重新安装docker

```shell
Examining /var/tmp/yum-root-agOWM7/docker-ce-17.03.2.ce-1.el7.centos.x86_64.rpm: docker-ce-17.03.2.ce-1.el7.centos.x86_64
Marking /var/tmp/yum-root-agOWM7/docker-ce-17.03.2.ce-1.el7.centos.x86_64.rpm to be installed
Resolving Dependencies
--> Running transaction check
---> Package docker-ce.x86_64 0:17.03.2.ce-1.el7.centos will be installed
--> Processing Dependency: docker-ce-selinux >= 17.03.2.ce-1.el7.centos for package: docker-ce-17.03.2.ce-1.el7.centos.x86_64
Loading mirror speeds from cached hostfile
 * base: mirrors.tuna.tsinghua.edu.cn
 * extras: mirrors.tuna.tsinghua.edu.cn
 * updates: mirrors.huaweicloud.com
Package docker-ce-selinux is obsoleted by docker-ce, but obsoleting package does not provide for requirements
--> Finished Dependency Resolution
Error: Package: docker-ce-17.03.2.ce-1.el7.centos.x86_64 (/docker-ce-17.03.2.ce-1.el7.centos.x86_64)
           Requires: docker-ce-selinux >= 17.03.2.ce-1.el7.centos
           Available: docker-ce-selinux-17.03.0.ce-1.el7.centos.noarch (docker-ce-stable)
               docker-ce-selinux = 17.03.0.ce-1.el7.centos
           Available: docker-ce-selinux-17.03.1.ce-0.1.rc1.el7.centos.noarch (docker-ce-test)
               docker-ce-selinux = 17.03.1.ce-0.1.rc1.el7.centos
           Available: docker-ce-selinux-17.03.1.ce-1.el7.centos.noarch (docker-ce-stable)
               docker-ce-selinux = 17.03.1.ce-1.el7.centos
           Available: docker-ce-selinux-17.03.2.ce-0.1.rc1.el7.centos.noarch (docker-ce-test)
               docker-ce-selinux = 17.03.2.ce-0.1.rc1.el7.centos
           Available: docker-ce-selinux-17.03.2.ce-1.el7.centos.noarch (docker-ce-stable)
               docker-ce-selinux = 17.03.2.ce-1.el7.centos
           Available: docker-ce-selinux-17.04.0.ce-0.1.rc1.el7.centos.noarch (docker-ce-test)
               docker-ce-selinux = 17.04.0.ce-0.1.rc1.el7.centos
           Available: docker-ce-selinux-17.04.0.ce-0.2.rc2.el7.centos.noarch (docker-ce-test)
               docker-ce-selinux = 17.04.0.ce-0.2.rc2.el7.centos
           Available: docker-ce-selinux-17.04.0.ce-1.el7.centos.noarch (docker-ce-edge)
               docker-ce-selinux = 17.04.0.ce-1.el7.centos
           Available: docker-ce-selinux-17.05.0.ce-0.1.rc1.el7.centos.noarch (docker-ce-test)
               docker-ce-selinux = 17.05.0.ce-0.1.rc1.el7.centos
           Available: docker-ce-selinux-17.05.0.ce-0.2.rc2.el7.centos.noarch (docker-ce-test)
               docker-ce-selinux = 17.05.0.ce-0.2.rc2.el7.centos
           Available: docker-ce-selinux-17.05.0.ce-0.3.rc3.el7.centos.noarch (docker-ce-test)
               docker-ce-selinux = 17.05.0.ce-0.3.rc3.el7.centos
           Available: docker-ce-selinux-17.05.0.ce-1.el7.centos.noarch (docker-ce-edge)
               docker-ce-selinux = 17.05.0.ce-1.el7.centos
 You could try using --skip-broken to work around the problem
```

#### `docker` 执行命令

```shell
[root@k8s-node-100 ~]# yum install https://mirrors.aliyun.com/docker-ce/linux/centos/7/x86_64/stable/Packages/docker-ce-selinux-17.03.2.ce-1.el7.centos.noarch.rpm -y
Loaded plugins: fastestmirror
docker-ce-selinux-17.03.2.ce-1.el7.centos.noarch.rpm                                                                                             |  28 kB  00:00:00

Examining /var/tmp/yum-root-agOWM7/docker-ce-selinux-17.03.2.ce-1.el7.centos.noarch.rpm: docker-ce-selinux-17.03.2.ce-1.el7.centos.noarch
Marking /var/tmp/yum-root-agOWM7/docker-ce-selinux-17.03.2.ce-1.el7.centos.noarch.rpm to be installed
Resolving Dependencies
--> Running transaction check
---> Package docker-ce-selinux.noarch 0:17.03.2.ce-1.el7.centos will be installed
--> Processing Conflict: docker-ce-selinux-17.03.2.ce-1.el7.centos.noarch conflicts docker-selinux
Loading mirror speeds from cached hostfile
 * base: mirrors.tuna.tsinghua.edu.cn
 * extras: mirrors.tuna.tsinghua.edu.cn
 * updates: mirrors.huaweicloud.com
--> Finished Dependency Resolution
Error: docker-ce-selinux conflicts with 2:container-selinux-2.55-1.el7.noarch
 You could try using --skip-broken to work around the problem
 You could try running: rpm -Va --nofiles --nodigest
[root@k8s-node-100 ~]# yum list container-selinux-2.55-1.el7.noarch
Loaded plugins: fastestmirror
Loading mirror speeds from cached hostfile
 * base: mirrors.tuna.tsinghua.edu.cn
 * extras: mirrors.tuna.tsinghua.edu.cn
 * updates: mirrors.huaweicloud.com
Installed Packages
container-selinux.noarch                                                              2:2.55-1.el7                                                               @extras

[root@k8s-node-100 ~]# yum erase container-selinux.noarch
Loaded plugins: fastestmirror
Resolving Dependencies
--> Running transaction check
---> Package container-selinux.noarch 2:2.55-1.el7 will be erased
--> Finished Dependency Resolution

Dependencies Resolved

========================================================================================================================================================================

 Package                                       Arch                               Version                                     Repository                           Size ========================================================================================================================================================================

Removing:
 container-selinux                             noarch                             2:2.55-1.el7                                @extras                              36 k

Transaction Summary
========================================================================================================================================================================

Remove  1 Package

Installed size: 36 k
Is this ok [y/N]: y
Downloading packages:
Running transaction check
Running transaction test
Transaction test succeeded
Running transaction
  Erasing    : 2:container-selinux-2.55-1.el7.noarch                                                                                                                1/1

  Verifying  : 2:container-selinux-2.55-1.el7.noarch                                                                                                                1/1


Removed:
  container-selinux.noarch 2:2.55-1.el7


Complete!
```





#### 服务器SSH免密

```shell
$ ssh-keygen -t rsa
$ cd /root/.ssh/ && cat id_rsa.pub >> authorized_keys 
$ cat id_rsa.pub
$ vi authorized_keys

ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQD3xuQB1+b2oFzVNLwf6bmOcWBZRLmJLJwDHVSCxVXqf0BtjRojK3v1RvebAs1IRY6uOh+oE2n41wWqb9Linxpcrq9SCmTe0cWxFi1NErFfknJlYHKARHg//K/7wcHAkPT0uL73jmwkRoBFTpetuCOk9uc5I8uSdoyC8NaSbc8RLFxllj/V3T6OIRaiTt2OP4gaCZgpOQeQRLv1ZZbEd+R9SY6DxVPEE1wl3f9OO00YvPuNiDKVS6B3LaEIAX0GWmk8axSo4ywO200pHbPHaEX3NEOjxpxCt7PVSaA6hMU7hMRlVG4FGRLfMyZu/pKkahC0oCJgyF3erXF+dW05fdoT root@k8s-node-100
```

#### 配置keepalive

##### keepalive 仅限在(`master`)主节点上，

##### 配置k8s-node-101节点

```shell
cat <<EOF > /etc/keepalived/keepalived.conf
global_defs {
   router_id LVS_k8s
}

vrrp_script CheckK8sMaster {
    script "curl -k https://192.168.1.111:6443"
    interval 3
    timeout 9
    fall 2
    rise 2
}

vrrp_instance VI_1 {
    state MASTER
    interface eno1
    virtual_router_id 61
    priority 100
    advert_int 1
    mcast_src_ip 192.168.1.101
    nopreempt
    authentication {
        auth_type PASS
        auth_pass sqP05dQgMSlzrxHj
    }
    unicast_peer {
        192.168.1.102
        192.168.1.103
    }
    virtual_ipaddress {
        192.168.1.111/24
    }
    track_script {
        CheckK8sMaster
    }
}
EOF
```

##### node-102 节点

```shell
cat <<EOF > /etc/keepalived/keepalived.conf
global_defs {
   router_id LVS_k8s
}

vrrp_script CheckK8sMaster {
    script "curl -k https://192.168.1.111:6443"
    interval 3
    timeout 9
    fall 2
    rise 2
}

vrrp_instance VI_1 {
    state BACKUP
    interface eno1
    virtual_router_id 61
    priority 90
    advert_int 1
    mcast_src_ip 192.168.1.102
    nopreempt
    authentication {
        auth_type PASS
        auth_pass sqP05dQgMSlzrxHj
    }
    unicast_peer {
        192.168.1.101
        192.168.1.103
    }
    virtual_ipaddress {
        192.168.1.111/24
    }
    track_script {
        CheckK8sMaster
    }
}
EOF
```

##### node-103

```shell
cat <<EOF > /etc/keepalived/keepalived.conf
global_defs {
   router_id LVS_k8s
}

vrrp_script CheckK8sMaster {
    script "curl -k https://192.168.1.111:6443"
    interval 3
    timeout 9
    fall 2
    rise 2
}

vrrp_instance VI_1 {
    state BACKUP
    interface eno1
    virtual_router_id 61
    priority 80
    advert_int 1
    mcast_src_ip 192.168.1.103
    nopreempt
    authentication {
        auth_type PASS
        auth_pass sqP05dQgMSlzrxHj
    }
    unicast_peer {
        192.168.1.101
        192.168.1.102
    }
    virtual_ipaddress {
        192.168.1.111/24
    }
    track_script {
        CheckK8sMaster
    }
}
EOF
```

最后达到的效果

```shell
[root@k8s-node-101 ~]# ip a
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host 
       valid_lft forever preferred_lft forever
2: eno1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether fc:4d:d4:f7:64:7b brd ff:ff:ff:ff:ff:ff
    inet 192.168.1.101/24 brd 192.168.1.255 scope global eno1
       valid_lft forever preferred_lft forever
    inet 192.168.1.111/24 scope global secondary eno1
       valid_lft forever preferred_lft forever
    inet6 fe80::fe4d:d4ff:fef7:647b/64 scope link 
       valid_lft forever preferred_lft forever
3: docker0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN group default 
    link/ether 02:42:a6:ec:3d:fe brd ff:ff:ff:ff:ff:ff
    inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0
       valid_lft forever preferred_lft forever

```

### 配置ETCD

#### node-101,设置cffsl环境

```shell
wget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64
wget https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64
wget https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64
chmod +x cfssl_linux-amd64
mv cfssl_linux-amd64 /usr/local/bin/cfssl
chmod +x cfssljson_linux-amd64
mv cfssljson_linux-amd64 /usr/local/bin/cfssljson
chmod +x cfssl-certinfo_linux-amd64
mv cfssl-certinfo_linux-amd64 /usr/local/bin/cfssl-certinfo
export PATH=/usr/local/bin:$PATH
```

#### 创建CA证书

```shell
mkdir /root/ssl
cd /root/ssl
cat >  ca-config.json <<EOF
{
"signing": {
"default": {
  "expiry": "8760h"
},
"profiles": {
  "kubernetes-Soulmate": {
    "usages": [
        "signing",
        "key encipherment",
        "server auth",
        "client auth"
    ],
    "expiry": "8760h"
  }
}
}
}
EOF

cat >  ca-csr.json <<EOF
{
"CN": "kubernetes-Soulmate",
"key": {
"algo": "rsa",
"size": 2048
},
"names": [
{
  "C": "CN",
  "ST": "shanghai",
  "L": "shanghai",
  "O": "k8s",
  "OU": "System"
}
]
}
EOF

cfssl gencert -initca ca-csr.json | cfssljson -bare ca

cat > etcd-csr.json <<EOF
{
  "CN": "etcd",
  "hosts": [
    "127.0.0.1",
    "192.168.1.101",
    "192.168.1.102",
    "192.168.1.103"
  ],
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "ST": "shanghai",
      "L": "shanghai",
      "O": "k8s",
      "OU": "System"
    }
  ]
}
EOF

cfssl gencert -ca=ca.pem \
  -ca-key=ca-key.pem \
  -config=ca-config.json \
  -profile=kubernetes-Soulmate etcd-csr.json | cfssljson -bare etcd
```

#### 证书分发到其他两个主节点上

```shell
mkdir -p /etc/etcd/ssl
cp etcd.pem etcd-key.pem ca.pem /etc/etcd/ssl/
ssh -n k8s-node-102 "mkdir -p /etc/etcd/ssl && exit"
ssh -n k8s-node-103 "mkdir -p /etc/etcd/ssl && exit"
scp -r /etc/etcd/ssl/*.pem k8s-node-102:/etc/etcd/ssl/
scp -r /etc/etcd/ssl/*.pem k8s-node-103:/etc/etcd/ssl/
```

#### 安装配置etcd（三个主节点）

##### 安装etcd

```shell
yum install etcd -y
mkdir -p /var/lib/etcd
```

##### k8s-node-101 配置 `etcd.service`

```shell
cat <<EOF >/etc/systemd/system/etcd.service
[Unit]
Description=Etcd Server
After=network.target
After=network-online.target
Wants=network-online.target
Documentation=https://github.com/coreos

[Service]
Type=notify
WorkingDirectory=/var/lib/etcd/
ExecStart=/usr/bin/etcd \
  --name k8s-node-101 \
  --cert-file=/etc/etcd/ssl/etcd.pem \
  --key-file=/etc/etcd/ssl/etcd-key.pem \
  --peer-cert-file=/etc/etcd/ssl/etcd.pem \
  --peer-key-file=/etc/etcd/ssl/etcd-key.pem \
  --trusted-ca-file=/etc/etcd/ssl/ca.pem \
  --peer-trusted-ca-file=/etc/etcd/ssl/ca.pem \
  --initial-advertise-peer-urls https://192.168.1.101:2380 \
  --listen-peer-urls https://192.168.1.101:2380 \
  --listen-client-urls https://192.168.1.101:2379,http://127.0.0.1:2379 \
  --advertise-client-urls https://192.168.1.101:2379 \
  --initial-cluster-token etcd-cluster-0 \
  --initial-cluster k8s-node-101=https://192.168.1.101:2380,k8s-node-102=https://192.168.1.102:2380,k8s-node-103=https://192.168.1.103:2380 \
  --initial-cluster-state new \
  --data-dir=/var/lib/etcd
Restart=on-failure
RestartSec=5
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
EOF
```

##### k8s-node-102 节点 `etc.service`

```shell
cat <<EOF >/etc/systemd/system/etcd.service
[Unit]
Description=Etcd Server
After=network.target
After=network-online.target
Wants=network-online.target
Documentation=https://github.com/coreos

[Service]
Type=notify
WorkingDirectory=/var/lib/etcd/
ExecStart=/usr/bin/etcd \
  --name k8s-node-102 \
  --cert-file=/etc/etcd/ssl/etcd.pem \
  --key-file=/etc/etcd/ssl/etcd-key.pem \
  --peer-cert-file=/etc/etcd/ssl/etcd.pem \
  --peer-key-file=/etc/etcd/ssl/etcd-key.pem \
  --trusted-ca-file=/etc/etcd/ssl/ca.pem \
  --peer-trusted-ca-file=/etc/etcd/ssl/ca.pem \
  --initial-advertise-peer-urls https://192.168.1.102:2380 \
  --listen-peer-urls https://192.168.1.102:2380 \
  --listen-client-urls https://192.168.1.102:2379,http://127.0.0.1:2379 \
  --advertise-client-urls https://192.168.1.102:2379 \
  --initial-cluster-token etcd-cluster-0 \
  --initial-cluster k8s-node-101=https://192.168.1.101:2380,k8s-node-102=https://192.168.1.102:2380,k8s-node-103=https://192.168.1.103:2380 \
  --initial-cluster-state new \
  --data-dir=/var/lib/etcd
Restart=on-failure
RestartSec=5
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
EOF
```

##### k8s-node-103 配置 etc.service

```shell
cat <<EOF >/etc/systemd/system/etcd.service
[Unit]
Description=Etcd Server
After=network.target
After=network-online.target
Wants=network-online.target
Documentation=https://github.com/coreos

[Service]
Type=notify
WorkingDirectory=/var/lib/etcd/
ExecStart=/usr/bin/etcd \
  --name k8s-node-103 \
  --cert-file=/etc/etcd/ssl/etcd.pem \
  --key-file=/etc/etcd/ssl/etcd-key.pem \
  --peer-cert-file=/etc/etcd/ssl/etcd.pem \
  --peer-key-file=/etc/etcd/ssl/etcd-key.pem \
  --trusted-ca-file=/etc/etcd/ssl/ca.pem \
  --peer-trusted-ca-file=/etc/etcd/ssl/ca.pem \
  --initial-advertise-peer-urls https://192.168.1.103:2380 \
  --listen-peer-urls https://192.168.1.103:2380 \
  --listen-client-urls https://192.168.1.103:2379,http://127.0.0.1:2379 \
  --advertise-client-urls https://192.168.1.103:2379 \
  --initial-cluster-token etcd-cluster-0 \
  --initial-cluster k8s-node-101=https://192.168.1.101:2380,k8s-node-102=https://192.168.1.102:2380,k8s-node-103=https://192.168.1.103:2380 \
  --initial-cluster-state new \
  --data-dir=/var/lib/etcd
Restart=on-failure
RestartSec=5
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
EOF
```



路径配置

```shell
mv /etc/systemd/system/etcd.service /usr/lib/systemd/system/

# 此时会提示冲突
[root@k8s-node-103 ~]# cat /usr/lib/systemd/system/etcd.service 
[Unit]
Description=Etcd Server
After=network.target
After=network-online.target
Wants=network-online.target

[Service]
Type=notify
WorkingDirectory=/var/lib/etcd/
EnvironmentFile=-/etc/etcd/etcd.conf
User=etcd
# set GOMAXPROCS to number of processors
ExecStart=/bin/bash -c "GOMAXPROCS=$(nproc) /usr/bin/etcd --name=\"${ETCD_NAME}\" --data-dir=\"${ETCD_DATA_DIR}\" --listen-client-urls=\"${ETCD_LISTEN_CLIENT_URLS}\""
Restart=on-failure
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target

 systemctl daemon-reload
 systemctl enable etcd
 systemctl start etcd
 systemctl status etcd

```

##### 在各自节点上执行检查命令

```shell
etcdctl --endpoints=https://192.168.1.101:2379,https://192.168.1.102:2379,https://192.168.1.103:2379 \
  --ca-file=/etc/etcd/ssl/ca.pem \
  --cert-file=/etc/etcd/ssl/etcd.pem \
  --key-file=/etc/etcd/ssl/etcd-key.pem  cluster-health

[root@k8s-node-103 ~]# etcdctl --endpoints=https://192.168.1.101:2379,https://192.168.1.102:2379,https://192.168.1.103:2379 \
>   --ca-file=/etc/etcd/ssl/ca.pem \
>   --cert-file=/etc/etcd/ssl/etcd.pem \
>   --key-file=/etc/etcd/ssl/etcd-key.pem  cluster-health
member 4469cb53324fe68b is healthy: got healthy result from https://192.168.1.102:2379
member 9f5e0acc1f346641 is healthy: got healthy result from https://192.168.1.101:2379
member e519401c4b995768 is healthy: got healthy result from https://192.168.1.103:2379
cluster is healthy

[root@k8s-node-102 ~]# etcdctl --endpoints=https://192.168.1.101:2379,https://192.168.1.102:2379,https://192.168.1.103:2379 \
>   --ca-file=/etc/etcd/ssl/ca.pem \
>   --cert-file=/etc/etcd/ssl/etcd.pem \
>   --key-file=/etc/etcd/ssl/etcd-key.pem  cluster-health
member 4469cb53324fe68b is healthy: got healthy result from https://192.168.1.102:2379
member 9f5e0acc1f346641 is healthy: got healthy result from https://192.168.1.101:2379
member e519401c4b995768 is healthy: got healthy result from https://192.168.1.103:2379
cluster is healthy

[root@k8s-node-101 ssl]# etcdctl --endpoints=https://192.168.1.101:2379,https://192.168.1.102:2379,https://192.168.1.103:2379 \
>   --ca-file=/etc/etcd/ssl/ca.pem \
>   --cert-file=/etc/etcd/ssl/etcd.pem \
>   --key-file=/etc/etcd/ssl/etcd-key.pem  cluster-health
member 4469cb53324fe68b is healthy: got healthy result from https://192.168.1.102:2379
member 9f5e0acc1f346641 is healthy: got healthy result from https://192.168.1.101:2379
member e519401c4b995768 is healthy: got healthy result from https://192.168.1.103:2379
cluster is healthy

```

#### Docker 安装命令：

```shell
$ yum install https://mirrors.aliyun.com/docker-ce/linux/centos/7/x86_64/stable/Packages/docker-ce-selinux-17.03.2.ce-1.el7.centos.noarch.rpm -y

$ yum list container-selinux-2.55-1.el7.noarch

$ yum erase container-selinux.noarch -y

$ yum install https://mirrors.aliyun.com/docker-ce/linux/centos/7/x86_64/stable/Packages/docker-ce-17.03.2.ce-1.el7.centos.x86_64.rpm -y
```

修改docker配置

```shell
vim /usr/lib/systemd/system/docker.service

ExecStart=/usr/bin/dockerd   -H tcp://0.0.0.0:2375 -H unix:///var/run/docker.sock  --registry-mirror=https://v0sh66ab.mirror.aliyuncs.com
```

docker启动

```shell
systemctl daemon-reload
systemctl restart docker
systemctl enable docker
systemctl status docker
```

启动的时候会报错：

```shell
Jun 14 04:34:48 k8s-node-95 dockerd[13013]: time="2018-06-14T04:34:48.583200130-04:00" level=warning msg="[!] DON'T BIND ON ANY IP ADDRESS WITHOUT setting -tlsverify IF YOU DON'T KNOW WHAT YOU'RE DOING [!]"
Jun 14 04:34:48 k8s-node-95 dockerd[13013]: time="2018-06-14T04:34:48.585672553-04:00" level=info msg="libcontainerd: new containerd process, pid: 13021"
Jun 14 04:34:49 k8s-node-95 dockerd[13013]: time="2018-06-14T04:34:49.591608230-04:00" level=error msg="[graphdriver] prior storage driver overlay2 failed: driver not supported"
Jun 14 04:34:49 k8s-node-95 dockerd[13013]: Error starting daemon: error initializing graphdriver: driver not supported
Jun 14 04:34:49 k8s-node-95 systemd[1]: docker.service: main process exited, code=exited, status=1/FAILURE
Jun 14 04:34:49 k8s-node-95 systemd[1]: Failed to start Docker Application Container Engine.
-- Subject: Unit docker.service has failed
-- Defined-By: systemd
-- Support: http://lists.freedesktop.org/mailman/listinfo/systemd-devel
```

解决办法：

```shell
mv /var/lib/docker /var/lib/docker.old  

vim /usr/lib/systemd/system/docker.service

ExecStart=/usr/bin/dockerd   -H tcp://0.0.0.0:2375 -H unix:///var/run/docker.sock  --registry-mirror=https://v0sh66ab.mirror.aliyuncs.com
```

之后再执行docker启动

```shell
systemctl daemon-reload
systemctl restart docker
systemctl enable docker
systemctl status docker
```

#### **docker使用阿里云镜像库加速**

注册阿里云开发者帐号帐号，`https://cr.console.aliyun.com/`

登陆后取得专属加速器地址，类似这样[https://xxxxxx.mirror.aliyuncs.com](https://xxxxxx.mirror.aliyuncs.com/)

### 安装、配置kubeadm

#### 1：所有节点安装kubelet kubeadm kubectl

```shell
yum install -y kubelet kubeadm kubectl
systemctl enable kubelet 
```

#### 2、所有节点修改kubelet配置文件

```shell
vi /etc/systemd/system/kubelet.service.d/10-kubeadm.conf

#修改这一行

Environment="KUBELET_CGROUP_ARGS=--cgroup-driver=cgroupfs"
Environment="KUBELET_EXTRA_ARGS=--v=2 --fail-swap-on=false --pod-infra-container-image=registry.cn-hangzhou.aliyuncs.com/k8sth/pause-amd64:3.0"
```

#### 3、所有节点配置文件重新加载

```shell
systemctl daemon-reload
systemctl enable kubelet
```

#### 4、命令补全 

```shell
yum install -y bash-completion
source /usr/share/bash-completion/bash_completion
source <(kubectl completion bash)
echo "source <(kubectl completion bash)" >> ~/.bashrc
```

### 初始化集群

#### 在 k8s-node-101、k8s-node-102、k8s-node-103 上添加配置文件(都相同）

```shell
cat <<EOF > config.yaml 
apiVersion: kubeadm.k8s.io/v1alpha1
kind: MasterConfiguration
etcd:
  endpoints:
  - https://192.168.1.101:2379
  - https://192.168.1.102:2379
  - https://192.168.1.103:2379
  caFile: /etc/etcd/ssl/ca.pem
  certFile: /etc/etcd/ssl/etcd.pem
  keyFile: /etc/etcd/ssl/etcd-key.pem
  dataDir: /var/lib/etcd
networking:
  podSubnet: 10.244.0.0/16
kubernetesVersion: 1.10.0
api:
  advertiseAddress: "192.168.1.111"
token: "b99a00.a144ef80536d4344"
tokenTTL: "0s"
apiServerCertSANs:
- k8s-node-95
- k8s-node-96
- k8s-node-97
- k8s-node-98
- k8s-node-99
- k8s-node-100
- k8s-node-101
- k8s-node-102
- k8s-node-103
- 192.168.1.95
- 192.168.1.95
- 192.168.1.96
- 192.168.1.97
- 192.168.1.98
- 192.168.1.99
- 192.168.1.100
- 192.168.1.101
- 192.168.1.102
- 192.168.1.103
featureGates:
  CoreDNS: true
imageRepository: "registry.cn-hangzhou.aliyuncs.com/k8sth"
EOF
```

#### 进行集群的初始化

- 配置文件定义podnetwork是`10.244.0.0/16` 
- `kubeadmin init –hlep`可以看出，`service`默认网段是`10.96.0.0/12` 
- `/etc/systemd/system/kubelet.service.d/10-kubeadm.conf`，默认dns地址`cluster-dns=10.96.0.10` 

```shell
[root@k8s-node-101 ~]# kubeadm init --config config.yaml 
[init] Using Kubernetes version: v1.10.0
[init] Using Authorization modes: [Node RBAC]
[preflight] Running pre-flight checks.
	[WARNING Service-Docker]: docker service is not enabled, please run 'systemctl enable docker.service'
	[WARNING FileExisting-crictl]: crictl not found in system path
Suggestion: go get github.com/kubernetes-incubator/cri-tools/cmd/crictl
[preflight] Starting the kubelet service
[certificates] Generated ca certificate and key.
[certificates] Generated apiserver certificate and key.
[certificates] apiserver serving cert is signed for DNS names [k8s-node-101 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local k8s-node-95 k8s-node-96 k8s-node-97 k8s-node-98 k8s-node-99 k8s-node-100 k8s-node-101 k8s-node-102 k8s-node-103] and IPs [10.96.0.1 192.168.1.111 192.168.1.95 192.168.1.95 192.168.1.96 192.168.1.97 192.168.1.98 192.168.1.99 192.168.1.100 192.168.1.101 192.168.1.102 192.168.1.103]
[certificates] Generated apiserver-kubelet-client certificate and key.
[certificates] Generated sa key and public key.
[certificates] Generated front-proxy-ca certificate and key.
[certificates] Generated front-proxy-client certificate and key.
[certificates] Valid certificates and keys now exist in "/etc/kubernetes/pki"
[kubeconfig] Wrote KubeConfig file to disk: "/etc/kubernetes/admin.conf"
[kubeconfig] Wrote KubeConfig file to disk: "/etc/kubernetes/kubelet.conf"
[kubeconfig] Wrote KubeConfig file to disk: "/etc/kubernetes/controller-manager.conf"
[kubeconfig] Wrote KubeConfig file to disk: "/etc/kubernetes/scheduler.conf"
[controlplane] Wrote Static Pod manifest for component kube-apiserver to "/etc/kubernetes/manifests/kube-apiserver.yaml"
[controlplane] Wrote Static Pod manifest for component kube-controller-manager to "/etc/kubernetes/manifests/kube-controller-manager.yaml"
[controlplane] Wrote Static Pod manifest for component kube-scheduler to "/etc/kubernetes/manifests/kube-scheduler.yaml"
[init] Waiting for the kubelet to boot up the control plane as Static Pods from directory "/etc/kubernetes/manifests".
[init] This might take a minute or longer if the control plane images have to be pulled.
[apiclient] All control plane components are healthy after 46.501688 seconds
[uploadconfig] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
[markmaster] Will mark node k8s-node-101 as master by adding a label and a taint
[markmaster] Master k8s-node-101 tainted and labelled with key/value: node-role.kubernetes.io/master=""
[bootstraptoken] Using token: b99a00.a144ef80536d4344
[bootstraptoken] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
[bootstraptoken] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
[bootstraptoken] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
[bootstraptoken] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
[addons] Applied essential addon: CoreDNS
[addons] Applied essential addon: kube-proxy

Your Kubernetes master has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

You can now join any number of machines by running the following on each node
as root:

  kubeadm join 192.168.1.111:6443 --token b99a00.a144ef80536d4344 --discovery-token-ca-cert-hash sha256:65d85b12cac62a6492f75e60b67904f0cbeaeefe618220166b174961e0f79d47

```

#### 执行命令

```shell
  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config
```

#### kubeadm生成证书密码文件分发到node-102和node-103上面去 

```shell
scp -r /etc/kubernetes/pki  k8s-node-103:/etc/kubernetes/
scp -r /etc/kubernetes/pki  k8s-node-102:/etc/kubernetes/
```

#### 安装flannel 网络，只需要在k8s-node-101执行

```shell
wget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
#版本信息：quay.io/coreos/flannel:v0.10.0-amd64

kubectl create -f  kube-flannel.yml

kubectl   get node

kubectl   get pods --all-namespaces
```

#### 

#### 部署dashboard 

##### kubernetes-dashboard.yaml

```yaml
# Copyright 2017 The Kubernetes Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# Configuration to deploy release version of the Dashboard UI compatible with
# Kubernetes 1.8.
#
# Example usage: kubectl create -f <this_file>

# ------------------- Dashboard Secret ------------------- #

apiVersion: v1
kind: Secret
metadata:
  labels:
    k8s-app: kubernetes-dashboard
  name: kubernetes-dashboard-certs
  namespace: kube-system
type: Opaque

---
# ------------------- Dashboard Service Account ------------------- #

apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    k8s-app: kubernetes-dashboard
  name: kubernetes-dashboard
  namespace: kube-system

---
# ------------------- Dashboard Role & Role Binding ------------------- #

kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: kubernetes-dashboard-minimal
  namespace: kube-system
rules:
  # Allow Dashboard to create 'kubernetes-dashboard-key-holder' secret.
- apiGroups: [""]
  resources: ["secrets"]
  verbs: ["create"]
  # Allow Dashboard to create 'kubernetes-dashboard-settings' config map.
- apiGroups: [""]
  resources: ["configmaps"]
  verbs: ["create"]
  # Allow Dashboard to get, update and delete Dashboard exclusive secrets.
- apiGroups: [""]
  resources: ["secrets"]
  resourceNames: ["kubernetes-dashboard-key-holder", "kubernetes-dashboard-certs"]
  verbs: ["get", "update", "delete"]
  # Allow Dashboard to get and update 'kubernetes-dashboard-settings' config map.
- apiGroups: [""]
  resources: ["configmaps"]
  resourceNames: ["kubernetes-dashboard-settings"]
  verbs: ["get", "update"]
  # Allow Dashboard to get metrics from heapster.
- apiGroups: [""]
  resources: ["services"]
  resourceNames: ["heapster"]
  verbs: ["proxy"]
- apiGroups: [""]
  resources: ["services/proxy"]
  resourceNames: ["heapster", "http:heapster:", "https:heapster:"]
  verbs: ["get"]

---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: kubernetes-dashboard-minimal
  namespace: kube-system
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: kubernetes-dashboard-minimal
subjects:
- kind: ServiceAccount
  name: kubernetes-dashboard
  namespace: kube-system

---
# ------------------- Dashboard Deployment ------------------- #

kind: Deployment
apiVersion: apps/v1beta2
metadata:
  labels:
    k8s-app: kubernetes-dashboard
  name: kubernetes-dashboard
  namespace: kube-system
spec:
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      k8s-app: kubernetes-dashboard
  template:
    metadata:
      labels:
        k8s-app: kubernetes-dashboard
    spec:
      nodeSelector:
        node-role.kubernetes.io/master: ""
      containers:
      - name: kubernetes-dashboard
        image: registry.cn-hangzhou.aliyuncs.com/k8sth/kubernetes-dashboard-amd64:v1.8.3
        ports:
        - containerPort: 8443
          protocol: TCP
        args:
          - --auto-generate-certificates
          # Uncomment the following line to manually specify Kubernetes API server Host
          # If not specified, Dashboard will attempt to auto discover the API server and connect
          # to it. Uncomment only if the default does not work.
          # - --apiserver-host=http://my-address:port
        volumeMounts:
        - name: kubernetes-dashboard-certs
          mountPath: /certs
          # Create on-disk volume to store exec logs
        - mountPath: /tmp
          name: tmp-volume
        livenessProbe:
          httpGet:
            scheme: HTTPS
            path: /
            port: 8443
          initialDelaySeconds: 30
          timeoutSeconds: 30
      volumes:
      - name: kubernetes-dashboard-certs
        secret:
          secretName: kubernetes-dashboard-certs
      - name: tmp-volume
        emptyDir: {}
      serviceAccountName: kubernetes-dashboard
      # Comment the following tolerations if Dashboard must not be deployed on master
      tolerations:
      - key: node-role.kubernetes.io/master
        effect: NoSchedule

---
# ------------------- Dashboard Service ------------------- #

kind: Service
apiVersion: v1
metadata:
  labels:
    k8s-app: kubernetes-dashboard
  name: kubernetes-dashboard
  namespace: kube-system
spec:
  type: NodePort
  ports:
    - port: 443
      targetPort: 8443
      nodePort: 30000
  selector:
    k8s-app: kubernetes-dashboard

---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: admin-user
  namespace: kube-system

---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  name: admin-user
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
- kind: ServiceAccount
  name: admin-user
  namespace: kube-system
```

```shell
# github地址：https://github.com/kubernetes/dashboard/releases/tag/v1.8.3
# 官方文档：https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/
wget   https://raw.githubusercontent.com/kubernetes/dashboard/v1.8.3/src/deploy/recommended/kubernetes-dashboard.yaml  
```

进行 yaml文件的下载和生成

获取Token，通过令牌登录

```shell
kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep admin-user | awk '{print $1}')
```

输入token，即可登录

```shell
https://192.168.150.181:30000/#!/login
```

registry.cn-hangzhou.aliyuncs.com/k8sth

强制删除 pod 命令，删除Pod一直处于Terminating状态 

```shell
kubectl delete pod  kube-flannel-aliyun-ds-46q98  --namespace=kube-system --grace-period=0 --force
```

coredns 未启动原因定位：

```shell
docker pull registry.cn-hangzhou.aliyuncs.com/kubernetes_containers/coredns
```

定位问题是镜像地址问题

编辑系统的pod内容

```shell
kubectl edit pod name -n kube-system
```

```shell
sed -i '9a\Environment="KUBELET_EXTRA_ARGS=–pod-infra-container-image=registry.cn-hangzhou.aliyuncs.com/osoulmate/pause-amd64:3.0"' /etc/systemd/system/kubelet.service.d/10-kubeadm.conf
```

```
Environment="KUBELET_KUBECONFIG_ARGS=--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf"
Environment="KUBELET_SYSTEM_PODS_ARGS=--pod-manifest-path=/etc/kubernetes/manifests --allow-privileged=true"
Environment="KUBELET_NETWORK_ARGS=--network-plugin=cni --cni-conf-dir=/etc/cni/net.d --cni-bin-dir=/opt/cni/bin"
Environment="KUBELET_DNS_ARGS=--cluster-dns=10.96.0.10 --cluster-domain=cluster.local"
Environment="KUBELET_AUTHZ_ARGS=--authorization-mode=Webhook --client-ca-file=/etc/kubernetes/pki/ca.pem"
Environment="KUBELET_CADVISOR_ARGS=--cadvisor-port=0"
Environment="KUBELET_CERTIFICATE_ARGS=--rotate-certificates=true --cert-dir=/var/lib/kubelet/pki"
Environment="KUBELET_EXTRA_ARGS=--node-labels=node-role.kubernetes.io/node='' --logtostderr=true --v=0"
ExecStart=
ExecStart=/usr/local/bin//kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_SYSTEM_PODS_ARGS $KUBELET_NETWORK_ARGS $KUBELET_DNS_ARGS $KUBELET_AUTHZ_ARGS $KUBELET_CADVISOR_ARGS $KUBELET_CERTIFICATE_ARGS $KUBELET_EXTRA_ARGS

```

### ETC 数据清理

```shell
# 使用环境变量定义api版本
export ETCDCTL_API=3

# etcd有目录结构类似linux文件系统，获取所有key看一看：
etcdctl get / --prefix --keys-only

# 查找etc
etcdctl get / --prefix --keys-only

# 删除
etcdctl del --prefix=true /registry

```

重新安装集群环境，我滴妈

```shell
yum remove kubelet kubeadm kubectl kubernetes-cni -y

yum install kubelet kubeadm kubectl kubernetes-cni -y

vim /etc/systemd/system/kubelet.service.d/10-kubeadm.conf

Environment="KUBELET_CGROUP_ARGS=--cgroup-driver=cgroupfs"

Environment="KUBELET_EXTRA_ARGS=--v=2 --fail-swap-on=false --pod-infra-container-image=registry.cn-hangzhou.aliyuncs.com/k8sth/pause-amd64:3.0"

systemctl daemon-reload

systemctl restart kubelet && systemctl status kubelet


kubeadm reset
#或
rm -rf /etc/kubernetes/*.conf
rm -rf /etc/kubernetes/manifests/*.yaml
docker ps -a |awk '{print $1}' |xargs docker rm -f
systemctl  stop kubelet

scp -r /etc/kubernetes/pki  k8s-node-102:/etc/kubernetes/

rm -rf $HOME/.kube

kubeadm init --config config.yaml 

mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

# 记住不执行时，此时会产生链接不上

```

报错

```shell
server.go:218] unable to load client CA file /etc/kubernetes/pki/ca.crt: open /etc/kubernetes/pki/ca.crt: no such file or directory

# 根据github上的说法。

You have to initialize the kubelet config before it will start successfully. That is typically done with kubeadm init or kubeadm join

# 需要运行kubeadm init 产生配置文件后，再启动kubelet才能成功，所以和这个报错可以忽略。
```

对于个人结点node首先配置master的kube文件夹的config文件到node结点



```shell
export image=pause-amd64:3.1
docker pull registry.cn-hangzhou.aliyuncs.com/anoy/${image}
docker tag registry.cn-hangzhou.aliyuncs.com/anoy/${image} k8s.gcr.io/${image}
docker rmi registry.cn-hangzhou.aliyuncs.com/anoy/${image}

export image=kube-proxy-amd64:v1.10.4
docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/${image}
docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/${image} k8s.gcr.io/${image}
docker rmi registry.cn-hangzhou.aliyuncs.com/google_containers/${image}
```



常用命令：

```shell
tail -f /var/log/messages

rm -rf /usr/local/bin/kubelet

cp /usr/bin/kubelet /usr/local/bin/

find / -name kubelet*

rm -rf /etc/systemd/system/multi-user.target.wants/kubelet.service
systemctl disable kubelet
```



错误：

```shell
server.go:218] unable to load client CA file /etc/kubernetes/pki/ca.crt: open /etc/kubernetes/pki/ca.crt: no such file or directory

根据github上的说法。

You have to initialize the kubelet config before it will start successfully. That is typically done with kubeadm init or kubeadm join

需要运行kubeadm init 产生配置文件后，再启动kubelet才能成功，所以和这个报错可以忽略。
```





其中发现特别奇怪的一种情况就是，莫名的会出现一个文件夹

```shell
[root@k8s-node-95 exec]# find / -name kubelet*
/etc/systemd/system/kubelet.service.d
/var/lib/kubelet
/usr/lib/systemd/system/kubelet.service
/usr/libexec/kubernetes/kubelet-plugins
/usr/local/bin/kubelet
[root@k8s-node-95 kubelet.service.d]# cd /var/lib/kubelet/
[root@k8s-node-95 kubelet]# ll
total 4
-rw-r--r-- 1 root root 40 Jun 17 02:37 cpu_manager_state
drwxr-xr-x 2 root root  6 Jun 17 02:37 device-plugins
drwxr-xr-x 2 root root 44 Jun 17 02:37 pki
drwx------ 2 root root  6 Jun 17 02:37 plugin-containers
drwxr-x--- 2 root root  6 Jun 17 02:37 plugins
drwxr-x--- 2 root root  6 Jun 17 02:37 pods
[root@k8s-node-95 kubelet]# rm -rf /var/lib/kubelet
[root@k8s-node-95 kubelet]# rm -rf /usr/lib/systemd/system/kubelet.service
[root@k8s-node-95 kubelet]# cd /usr/libexec/kubernetes/
[root@k8s-node-95 kubernetes]# ll
total 0
drwxr-xr-x. 3 root root 20 Jun 14 06:45 kubelet-plugins
[root@k8s-node-95 kubernetes]# rm -rf kubelet-plugins/
[root@k8s-node-95 kubernetes]# rm -rf /usr/local/bin/kubelet
[root@k8s-node-95 kubernetes]# 

```





```shell
export image=pause-amd64:3.1
docker pull registry.cn-hangzhou.aliyuncs.com/anoy/${image}
docker tag registry.cn-hangzhou.aliyuncs.com/anoy/${image} k8s.gcr.io/${image}
docker rmi registry.cn-hangzhou.aliyuncs.com/anoy/${image}

export image=kube-proxy-amd64:v1.10.4
docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/${image}
docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/${image} k8s.gcr.io/${image}
docker rmi registry.cn-hangzhou.aliyuncs.com/google_containers/${image}

yum remove kubelet kubeadm kubectl kubernetes-cni -y

yum install kubelet kubeadm kubectl kubernetes-cni -y

vim /etc/systemd/system/kubelet.service.d/10-kubeadm.conf

Environment="KUBELET_CGROUP_ARGS=--cgroup-driver=cgroupfs"

Environment="KUBELET_EXTRA_ARGS=--v=2 --fail-swap-on=false --pod-infra-container-image=registry.cn-hangzhou.aliyuncs.com/k8sth/pause-amd64:3.0"

systemctl daemon-reload && systemctl restart kubelet && systemctl status kubelet

systemctl stop kubelet && systemctl status kubelet

kubeadm reset

kubeadm join 192.168.1.111:6443 --token srw09q.8xuzjaroxj6yjul3 --discovery-token-ca-cert-hash sha256:0209586f10f119722fa129dedce68b10f7043ace922c23ea0b4231b8c1280cd5

systemctl start kubelet && tail -f /var/log/messages
```



参考链接：

- https://www.kubernetes.org.cn/3808.html